{"role": "user", "content": "Okay, I think things are pretty stable at the moment. I'm watching an interesting talk on YouTube (https://www.youtube.com/watch?v=n1E9IZfvGMA), here's the transcript, what do you think? What exactly are we scaling?\n0:00\nWe talked three years ago. In your view, what has  been the biggest update over the last three years?  What has been the biggest difference  between what it felt like then versus now? \n0:10\nBroadly speaking, the exponential of the  underlying technology has gone about as  \n0:18\nI expected it to go. There's plus or minus   a year or two here and there. I don't know that I would've  \n0:27\npredicted the specific direction of code. But when I look at the exponential,  \n0:34\nit is roughly what I expected in terms of  the march of the models from smart high  \n0:39\nschool student to smart college student to  beginning to do PhD and professional stuff,  \n0:44\nand in the case of code reaching beyond that. The frontier is a little bit uneven, but it's   roughly what I expected. What has been the most surprising  \n0:55\nthing is the lack of public recognition of how  close we are to the end of the exponential. \n1:02\nTo me, it is absolutely wild that you have  people \u2014 within the bubble and outside the  \n1:09\nbubble \u2014 talking about the same tired, old  hot-button political issues, when we are  \n1:19\nnear the end of the exponential. I want to understand what that   exponential looks like right now. The first question I asked you when  \n1:27\nwe recorded three years ago was, \"what\u2019s  up with scaling and why does it work?\"  I have a similar question now,  but it feels more complicated. \n1:35\nAt least from the public's point of view, three  years ago there were well-known public trends  \n1:41\nacross many orders of magnitude of compute  where you could see how the loss improves.  Now we have RL scaling and there's  no publicly known scaling law for it. \n1:49\nIt's not even clear what the story is. Is this supposed to be teaching the model skills?  Is it supposed to be teaching meta-learning? What is the scaling hypothesis at this point? \n1:59\nI actually have the same hypothesis  I had even all the way back in 2017. \n2:06\nI think I talked about it last time, but I wrote  a doc called \"The Big Blob of Compute Hypothesis\". \n2:12\nIt wasn't about the scaling of  language models in particular.  When I wrote it GPT-1 had just come out. That was one among many things. \n2:22\nBack in those days there was robotics. People tried to work on reasoning as   a separate thing from language models,  and there was scaling of the kind of RL  \n2:30\nthat happened in AlphaGo and in Dota at OpenAI. People remember StarCraft at DeepMind, AlphaStar. \n2:43\nIt was written as a more general document. Rich Sutton put out \"The Bitter  \n2:52\nLesson\" a couple years later. The hypothesis is basically the same. \n2:57\nWhat it says is that all the cleverness, all the  techniques, all the \"we need a new method to do  \n3:04\nsomething\", that doesn't matter very much. There are only a few things that matter.  I think I listed seven of them. One is how much raw compute you have. \n3:13\nThe second is the quantity of data. The third is the quality and distribution of data. \n3:20\nIt needs to be a broad distribution. The fourth is how long you train for. \n3:27\nThe fifth is that you need an objective  function that can scale to the moon.  The pre-training objective function  is one such objective function. \n3:36\nAnother is the RL objective  function that says you have a goal,  \n3:42\nyou're going to go out and reach the goal. Within that, there's objective rewards like  \n3:48\nyou see in math and coding, and there's  more subjective rewards like you see in   RLHF or higher-order versions of that. Then the sixth and seventh were things  \n3:59\naround normalization or conditioning,  just getting the numerical stability  \n4:04\nso that the big blob of compute flows in this  laminar way instead of running into problems. \n4:11\nThat was the hypothesis, and  it's a hypothesis I still hold.  I don't think I've seen very  much that is not in line with it. \n4:21\nThe pre-training scaling laws were one example  of what we see there. Those have continued going.  \n4:31\nNow it's been widely reported,  we feel good about pre-training.  It\u2019s continuing to give us gains. What has changed is that now we're  \n4:41\nalso seeing the same thing for RL. We're seeing a pre-training phase   and then an RL phase on top of that. With RL, it\u2019s actually just the same. \n4:55\nEven other companies have published things in  some of their releases that say, \"We train the  \n5:05\nmodel on math contests \u2014 AIME or other things  \u2014 and how well the model does is log-linear in  \n5:14\nhow long we've trained it.\" We see that as well,   and it's not just math contests. It's a wide variety of RL tasks. \n5:21\nWe're seeing the same scaling in  RL that we saw for pre-training. \n5:27\nYou mentioned Rich Sutton and \"The Bitter Lesson\". I interviewed him last year,   and he's actually very non-LLM-pilled. I don\u2019t know if this is his perspective,  \n5:38\nbut one way to paraphrase his objection is:  Something which possesses the true core of human  \n5:44\nlearning would not require all these billions  of dollars of data and compute and these bespoke  \n5:51\nenvironments, to learn how to use Excel, how to  use PowerPoint, how to navigate a web browser. \n5:57\nThe fact that we have to build in these skills  using these RL environments hints that we are  \n6:04\nactually lacking a core human learning algorithm. So we're scaling the wrong thing. That does raise  \n6:13\nthe question. Why are we doing all this RL scaling  if we think there's something that's going to be   human-like in its ability to learn on the fly? I think this puts together several things that  \n6:23\nshould be thought of differently. There is a genuine puzzle here,  \n6:29\nbut it may not matter. In fact, I would guess it probably   doesn't matter. There is an interesting thing. Let  me take the RL out of it for a second, because I  \n6:37\nactually think it's a red herring to say that RL  is any different from pre-training in this matter. \n6:43\nIf we look at pre-training  scaling, it was very interesting   back in 2017 when Alec Radford was doing GPT-1. The models before GPT-1 were trained on datasets  \n6:59\nthat didn't represent a wide distribution of text. You had very standard language  \n7:08\nmodeling benchmarks. GPT-1 itself was trained on   a bunch of fanfiction, I think actually. It was literary text, which is a very  \n7:17\nsmall fraction of the text you can get. In those days it was like a billion words  \n7:23\nor something, so small datasets representing  a pretty narrow distribution of what you can  \n7:32\nsee in the world. It didn't generalize well.  If you did better on some fanfiction corpus,  \n7:43\nit wouldn't generalize that well to other  tasks. We had all these measures. We had   all these measures of how well it did at  predicting all these other kinds of texts. \n7:55\nIt was only when you trained over all the tasks  on the internet \u2014 when you did a general internet  \n8:01\nscrape from something like Common Crawl or  scraping links in Reddit, which is what we did for  \n8:06\nGPT-2 \u2014 that you started to get generalization. I think we're seeing the same thing on RL. \n8:15\nWe're starting first with simple RL tasks like  training on math competitions, then moving to  \n8:24\nbroader training that involves things like code. Now we're moving to many other tasks. \n8:31\nI think then we're going to  increasingly get generalization.  So that kind of takes out the  RL vs. pre-training side of it. \n8:39\nBut there is a puzzle either way, which is that  in pre-training we use trillions of tokens. \n8:50\nHumans don't see trillions of words. So there is an actual sample   efficiency difference here. There is actually something different here. \n8:59\nThe models start from scratch  and they need much more training. \n9:06\nBut we also see that once they're trained,  if we give them a long context length of  \n9:15\na million \u2014 the only thing blocking long  context is inference \u2014 they're very good at   learning and adapting within that context. So I don\u2019t know the full answer to this. \n9:24\nI think there's something going  on where pre-training is not like   the process of humans learning, but it's  somewhere between the process of humans  \n9:32\nlearning and the process of human evolution. We get many of our priors from evolution. \n9:38\nOur brain isn't just a blank slate. Whole books have been written about this.  The language models are  much more like blank slates. \n9:45\nThey literally start as random weights, whereas  the human brain starts with all these regions   connected to all these inputs and outputs. Maybe we should think of pre-training \u2014 and  \n9:56\nfor that matter, RL as well \u2014 as something  that exists in the middle space between  \n10:02\nhuman evolution and human on-the-spot learning. And we should think of the in-context learning  \n10:10\nthat the models do as something between long-term  human learning and short-term human learning.  \n10:17\nSo there's this hierarchy. There\u2019s evolution,  there's long-term learning, there's short-term  \n10:22\nlearning, and there's just human reaction. The LLM phases exist along this spectrum,  \n10:28\nbut not necessarily at exactly the same points. There\u2019s no analog to some of the human modes  \n10:34\nof learning the LLMs are falling in  between the points. Does that make sense? \n10:40\nYes, although some things  are still a bit confusing.  For example, if the analogy is that this  is like evolution so it's fine that it's  \n10:45\nnot sample efficient, then if we're  going to get super sample-efficient  \n10:51\nagent from in-context learning, why are we  bothering to build all these RL environments? \n10:56\nThere are companies whose work seems to  be teaching models how to use this API,   how to use Slack, how to use whatever. It's confusing to me why there's so much emphasis  \n11:04\non that if the kind of agent that can just learn  on the fly is emerging or has already emerged. \n11:11\nI can't speak for the emphasis of anyone else. I can only talk about how we think about it. \n11:20\nThe goal is not to teach the model  every possible skill within RL,   just as we don't do that within pre-training. Within pre-training, we're not trying to expose  \n11:29\nthe model to every possible way  that words could be put together. \n11:37\nRather, the model trains on a lot of things and  then reaches generalization across pre-training. \n11:43\nThat was the transition from GPT-1 to GPT-2 that  I saw up close. The model reaches a point. I had  \n11:53\nthese moments where I was like, \"Oh yeah, you  just give the model a list of numbers \u2014 this is  \n12:01\nthe cost of the house, this is the square feet of  the house \u2014 and the model completes the pattern   and does linear regression.\" Not great, but it does it,  \n12:08\nand it's never seen that exact thing before. So to the extent that we are building these  \n12:16\nRL environments, the goal is very similar to what  was done five or ten years ago with pre-training. \n12:26\nWe're trying to get a whole bunch of data, not  because we want to cover a specific document or a  \n12:32\nspecific skill, but because we want to generalize. I think the framework you're laying down obviously  \nIs diffusion cope?\n12:39\nmakes sense. We're making progress toward AGI.  Nobody at this point disagrees we're going to  \n12:46\nachieve AGI this century. The crux is you say we're   hitting the end of the exponential. Somebody else looks at this and says,  \n12:55\n\"We've been making progress since 2012,  and by 2035 we'll have a human-like agent.\" \n13:04\nObviously we\u2019re seeing in these models  the kinds of things that evolution did,   or that learning within a human lifetime does. I want to understand what you\u2019re seeing  \n13:11\nthat makes you think it's one  year away and not ten years away. \n13:17\nThere are two claims you could make  here, one stronger and one weaker. \n13:26\nStarting with the weaker claim, when  I first saw the scaling back in 2019,  \n13:35\nI wasn\u2019t sure. This was a 50/50 thing. I  thought I saw something. My claim was that this  \n13:43\nwas much more likely than anyone thinks. Maybe there's a 50% chance this happens. \n13:51\nOn the basic hypothesis of, as you put it, within  ten years we'll get to what I call a \"country of  \n14:00\ngeniuses in a data center\", I'm at 90% on that. It's hard to go much higher than 90%  \n14:06\nbecause the world is so unpredictable. Maybe the irreducible uncertainty puts us at 95%,  \n14:13\nwhere you get to things like multiple companies  having internal turmoil, Taiwan gets invaded,  \n14:24\nall the fabs get blown up by missiles. Now you've jinxed us, Dario. \n14:30\nYou could construct a 5% world where  things get delayed for ten years. \n14:43\nThere's another 5% which is that I'm very  confident on tasks that can be verified. \n14:50\nWith coding, except for that  irreducible uncertainty,   I think we'll be there in one or two years. There's no way we will not be there in ten years  \n14:58\nin terms of being able to do end-to-end coding. My one little bit of fundamental uncertainty,  \n15:05\neven on long timescales, is about tasks that  aren't verifiable: planning a mission to Mars;  \n15:14\ndoing some fundamental scientific  discovery like CRISPR; writing a novel. \n15:21\nIt\u2019s hard to verify those tasks. I am almost certain we have a  \n15:27\nreliable path to get there, but if there's  a little bit of uncertainty it's there. \n15:34\nOn the ten-year timeline I'm at 90%,  which is about as certain as you can be. \n15:40\nI think it's crazy to say that  this won't happen by 2035. \n15:46\nIn some sane world, it would  be outside the mainstream.  But the emphasis on verification hints to me a  lack of belief that these models are generalized. \n15:58\nIf you think about humans, we're both good  at things for which we get verifiable reward   and things for which we don't. No, this is why I\u2019m almost sure. \n16:07\nWe already see substantial generalization  from things that verify to things that   don't. We're already seeing that. But it seems like you were emphasizing  \n16:15\nthis as a spectrum which will split apart  which domains in which we see more progress. \n16:21\nThat doesn't seem like how humans get better. The world in which we don't get there is the world  \n16:27\nin which we do all the verifiable things. Many of them generalize,  \n16:34\nbut we don't fully get there. We don\u2019t fully color in the other side  \n16:40\nof the box. It's not a binary thing. Even if generalization is weak and you can only do  \n16:47\nverifiable domains, it's not clear to me you could  automate software engineering in such a world.  You are \"a software engineer\" in some sense, but  part of being a software engineer for you involves  \n16:58\nwriting long memos about your grand vision. I don\u2019t think that\u2019s part of the job of SWE. \n17:03\nThat's part of the job of the  company, not SWE specifically.  But SWE does involve design  documents and other things like that. \n17:10\nThe models are already pretty  good at writing comments.  Again, I\u2019m making much weaker claims here than  I believe, to distinguish between two things. \n17:24\nWe're already almost there  for software engineering.  By what metric? There's one metric which is  how many lines of code are written by AI. \n17:32\nIf you consider other productivity improvements  in the history of software engineering,   compilers write all the lines of software. There's a difference between how many lines  \n17:40\nare written and how big the productivity  improvement is. \"We\u2019re almost there\" meaning\u2026  \n17:47\nHow big is the productivity improvement,  not just how many lines are written by AI?  I actually agree with you on this. I've made a series of predictions on  \n17:57\ncode and software engineering. I think people have repeatedly misunderstood them. \n18:03\nLet me lay out the spectrum. About eight or nine months ago,  \n18:09\nI said the AI model will be writing 90% of  the lines of code in three to six months. \n18:16\nThat happened, at least at some places. It happened at Anthropic, happened with   many people downstream using our models. But that's actually a very weak criterion. \n18:27\nPeople thought I was saying that we won't need 90%  of the software engineers. Those things are worlds  \n18:32\napart. The spectrum is: 90% of code is written by  the model, 100% of code is written by the model. \n18:41\nThat's a big difference in productivity. 90% of the end-to-end SWE tasks \u2014 including  \n18:47\nthings like compiling, setting up clusters  and environments, testing features,  \n18:54\nwriting memos \u2014 are done by the models. 100%  of today's SWE tasks are done by the models. \n19:02\nEven when that happens, it doesn't mean  software engineers are out of a job.  There are new higher-level things  they can do, where they can manage. \n19:10\nThen further down the spectrum, there's  90% less demand for SWEs, which I think  \n19:15\nwill happen but this is a spectrum. I wrote about it in \"The Adolescence  \n19:21\nof Technology\" where I went through  this kind of spectrum with farming. \n19:26\nI actually totally agree with you on that.  These are very different  benchmarks from each other,  \n19:32\nbut we're proceeding through them super fast. Part of your vision is that going from 90 to 100  \n19:38\nis going to happen fast, and that it  leads to huge productivity improvements. \n19:45\nBut what I notice is that even in greenfield  projects people start with Claude Code or   something, people report starting a lot of  projects\u2026 Do we see in the world out there  \n19:54\na renaissance of software, all these new  features that wouldn't exist otherwise?  At least so far, it doesn't seem like we see that. So that does make me wonder. \n20:02\nEven if I never had to intervene with  Claude Code, the world is complicated.  \n20:09\nJobs are complicated. Closing the loop on  self-contained systems, whether it\u2019s just  \n20:14\nwriting software or something, how much  broader gains would we see just from that? \n20:20\nMaybe that should dilute our estimation  of the \"country of geniuses\".  I simultaneously agree with you that it's a  reason why these things don't happen instantly,  \n20:35\nbut at the same time, I think  the effect is gonna be very fast. \n20:41\nYou could have these two poles. One is that AI is not going to make  \n20:47\nprogress. It's slow. It's going to take  forever to diffuse within the economy. \n20:52\nEconomic diffusion has become one of  these buzzwords that's a reason why   we're not going to make AI progress,  or why AI progress doesn't matter. \n21:00\nThe other axis is that we'll get recursive  self-improvement, the whole thing.  Can't you just draw an  exponential line on the curve? \n21:08\nWe're going to have Dyson spheres around the  sun so many nanoseconds after we get recursive. \n21:17\nI'm completely caricaturing the view  here, but there are these two extremes. \n21:23\nBut what we've seen from the beginning, at least  if you look within Anthropic, there's this bizarre  \n21:30\n10x per year growth in revenue that we've seen. So in 2023, it was zero to $100 million. \n21:38\nIn 2024, it was $100 million to $1 billion. In 2025, it was $1 billion to $ 9-10 billion. \n21:46\nYou guys should have just bought a billion  dollars of your own products so you could just\u2026  And the first month of this  year, that exponential is... \n21:54\nYou would think it would slow down, but we  added another few billion to revenue in January. \n22:05\nObviously that curve can't go on forever. The GDP is only so large.  I would even guess that it bends somewhat this  year, but that is a fast curve. That's a really  \n22:20\nfast curve. I would bet it stays pretty fast  even as the scale goes to the entire economy. \n22:25\nSo I think we should be thinking about this middle  world where things are extremely fast, but not  \n22:34\ninstant, where they take time because of economic  diffusion, because of the need to close the loop. \n22:39\nBecause it's fiddly: \"I have to do change  management within my enterprise\u2026 I set this up,  \n22:50\nbut I have to change the security permissions  on this in order to make it actually work\u2026  \n22:55\nI had this old piece of software that  checks the model before it's compiled  \n23:01\nand released and I have to rewrite it. Yes, the model can do that, but I have   to tell the model to do that. It has to take time to do that.\" \n23:10\nSo I think everything we've seen so far is  compatible with the idea that there's one fast  \n23:17\nexponential that's the capability of the model. Then there's another fast exponential   that's downstream of that, which is the  diffusion of the model into the economy. \n23:26\nNot instant, not slow, much faster than any  previous technology, but it has its limits. \n23:37\nWhen I look inside Anthropic, when I look at our  customers: fast adoption, but not infinitely fast. \n23:44\nCan I try a hot take on you? Yeah.  I feel like diffusion is cope that people say. When the model isn't able to do something,  \n23:51\nthey're like, \"oh, but it's a diffusion issue.\" But then you should use the comparison to humans. \n23:56\nYou would think that the inherent advantages  that AIs have would make diffusion a much easier   problem for new AIs getting onboarded  than new humans getting onboarded. \n24:06\nAn AI can read your entire  Slack and your drive in minutes.  They can share all the knowledge that the  other copies of the same instance have. \n24:12\nYou don't have this adverse selection  problem when you're hiring AI, so you   can just hire copies of a vetted AI model. Hiring a human is so much more of a hassle. \n24:20\nPeople hire humans all the time. We pay humans upwards of $50 trillion   in wages because they're useful, even though in  principle it would be much easier to integrate  \n24:29\nAIs into the economy than it is to hire  humans. The diffusion doesn't really explain.  I think diffusion is very real  and doesn't exclusively have  \n24:41\nto do with limitations on the AI models. Again, there are people who use diffusion  \n24:49\nas kind of a buzzword to say this isn't a  big deal. I'm not talking about that. I'm   not talking about how AI will diffuse  at the speed of previous technologies. \n24:58\nI think AI will diffuse much faster than previous  technologies have, but not infinitely fast. \n25:04\nI'll just give an example of this. There's Claude  Code. Claude Code is extremely easy to set up. \n25:10\nIf you're a developer, you can  just start using Claude Code.  There is no reason why a developer at a  large enterprise should not be adopting  \n25:19\nClaude Code as quickly as an individual  developer or developer at a startup. \n25:25\nWe do everything we can to promote it. We sell Claude Code to enterprises. \n25:31\nBig enterprises, big financial companies, big  pharmaceutical companies, all of them are adopting  \n25:38\nClaude Code much faster than enterprises  typically adopt new technology. But again,  \n25:46\nit takes time. Any given feature or any given  product, like Claude Code or Cowork, will get  \n25:54\nadopted by the individual developers who are on  Twitter all the time, by the Series A startups,  \n26:02\nmany months faster than they will get adopted  by a large enterprise that does food sales. \n26:11\nThere are just a number of factors. You have to go through legal,   you have to provision it for everyone. It has to pass security and compliance. \n26:20\nThe leaders of the company who are further away  from the AI revolution are forward-looking,  \n26:26\nbut they have to say, \"Oh, it makes  sense for us to spend 50 million.  This is what this Claude Code thing is. This is why it helps our company. \n26:35\nThis is why it makes us more productive.\" Then they have to explain   to the people two levels below. They have to say, \"Okay, we have 3,000 developers. \n26:42\nHere's how we're going to roll  it out to our developers.\"  We have conversations like this every day. We are doing everything we can to make  \n26:50\nAnthropic's revenue grow 20 or  30x a year instead of 10x a year. \n26:57\nAgain, many enterprises are just  saying, \"This is so productive.  We're going to take shortcuts in  our usual procurement process.\" \n27:05\nThey're moving much faster than  when we tried to sell them just   the ordinary API, which many of them use. Claude Code is a more compelling product,  \n27:13\nbut it's not an infinitely compelling product. I don't think even AGI or powerful AI or  \n27:19\n\"country of geniuses in a data center\"  will be an infinitely compelling product.  It will be a compelling product enough maybe to  get 3-5x, or 10x, a year of growth, even when  \n27:28\nyou're in the hundreds of billions of dollars,  which is extremely hard to do and has never been   done in history before, but not infinitely fast. I buy that it would be a slight slowdown. \n27:36\nMaybe this is not your claim, but  sometimes people talk about this like,   \"Oh, the capabilities are there, but because of  diffusion... otherwise we're basically at AGI\". \n27:46\nI don't believe we're basically at AGI. I think if you had the \"country   of geniuses in a data center\"... If we had the \"country of geniuses  \n27:53\nin a data center\", we would know it. We would know it if you had the   \"country of geniuses in a data center\". Everyone in this room would know it. \n28:01\nEveryone in Washington would know it. People in rural parts might not know it,  \n28:07\nbut we would know it. We don't  have that now. That is very clear. \nIs continual learning necessary?\n29:42\nComing back to concrete prediction\u2026 Because there  are so many different things to disambiguate,   it can be easy to talk past each other  when we're talking about capabilities. \n29:50\nFor example, when I interviewed you three  years ago, I asked you a prediction about what   we should expect three years from now. You were  right. You said, \"We should expect systems which,  \n30:00\nif you talk to them for the course of an  hour, it's hard to tell them apart from   a generally well-educated human.\" I think you were right about that. \n30:07\nI think spiritually I feel unsatisfied because my  internal expectation was that such a system could  \n30:13\nautomate large parts of white-collar work. So it might be more productive to talk about   the actual end capabilities  you want from such a system. \n30:21\nI will basically tell you where I think we are. Let me ask a very specific question so that  \n30:28\nwe can figure out exactly what kinds of  capabilities we should think about soon.  Maybe I'll ask about it in the context of a job  I understand well, not because it's the most  \n30:36\nrelevant job, but just because I can evaluate  the claims about it. Take video editors. I have  \n30:42\nvideo editors. Part of their job involves  learning about our audience's preferences,  \n30:47\nlearning about my preferences and tastes,  and the different trade-offs we have.  They\u2019re, over the course of many months,  building up this understanding of context. \n30:55\nThe skill and ability they have six  months into the job, a model that can   pick up that skill on the job on the fly,  when should we expect such an AI system? \n31:04\nI guess what you're talking about is that  we're doing this interview for three hours. \n31:09\nSomeone's going to come in,  someone's going to edit it.  They're going to be like, \"Oh, I don't know, Dario  scratched his head and we could edit that out.\" \n31:19\n\"Magnify that.\" \"There was this long   discussion that is less interesting to people. There's another thing that's more interesting  \n31:27\nto people, so let's make this edit.\" I think the \"country of geniuses in  \n31:33\na data center\" will be able to do that. The way it will be able to do that is it will   have general control of a computer screen. You'll be able to feed this in. \n31:43\nIt'll be able to also use the computer screen  to go on the web, look at all your previous  \n31:49\ninterviews, look at what people are saying  on Twitter in response to your interviews,   talk to you, ask you questions, talk to  your staff, look at the history of edits  \n31:59\nthat you did, and from that, do the job. I think that's dependent on several things. \n32:06\nI think this is one of the things  that's actually blocking deployment:   getting to the point on computer use where the  models are really masters at using the computer. \n32:16\nWe've seen this climb in benchmarks, and  benchmarks are always imperfect measures.  But I think when we first released computer use a  year and a quarter ago, OSWorld was at maybe 15%. \n32:33\nI don't remember exactly, but  we've climbed from that to 65-70%. \n32:40\nThere may be harder measures as well, but I think  computer use has to pass a point of reliability. \n32:46\nCan I just follow up on that before  you move on to the next point?  For years, I've been trying to build  different internal LLM tools for myself. \n32:54\nOften I have these text-in, text-out  tasks, which should be dead center  \n32:59\nin the repertoire of these models. Yet I still hire humans to do them.  If it's something like, \"identify what the  best clips would be in this transcript\",  \n33:07\nmaybe the LLMs do a seven-out-of-ten job on them. But there's not this ongoing way I can engage  \n33:12\nwith them to help them get better at the  job the way I could with a human employee.  That missing ability, even if you  solve computer use, would still block  \n33:20\nmy ability to offload an actual job to them. This gets back to what we were talking about  \n33:28\nbefore with learning on the job. It's very  interesting. I think with the coding agents,  \n33:34\nI don't think people would say that learning on  the job is what is preventing the coding agents  \n33:39\nfrom doing everything end to end. They  keep getting better. We have engineers  \n33:46\nat Anthropic who don't write any code. When I look at the productivity, to your  \n33:51\nprevious question, we have folks who say, \"This  GPU kernel, this chip, I used to write it myself. \n33:58\nI just have Claude do it.\" There's this enormous improvement in productivity. \n34:04\nWhen I see Claude Code, familiarity with  the codebase or a feeling that the model  \n34:13\nhasn't worked at the company for a year, that's  not high up on the list of complaints I see. \n34:18\nI think what I'm saying is that we're  kind of taking a different path.  Don't you think with coding that's because there  \n34:24\nis an external scaffold of memory which  exists instantiated in the codebase?  I don't know how many other jobs have that. Coding made fast progress precisely because  \n34:33\nit has this unique advantage that  other economic activity doesn't.  But when you say that, what you're implying is  that by reading the codebase into the context,  \n34:44\nI have everything that the human  needed to learn on the job.  So that would be an example of\u2014whether it's  written or not, whether it's available or  \n34:54\nnot\u2014a case where everything you needed  to know you got from the context window. \n35:00\nWhat we think of as learning\u2014\"I started this job,  it's going to take me six months to understand the   code base\"\u2014the model just did it in the context. I honestly don't know how to think about  \n35:09\nthis because there are people who  qualitatively report what you're saying. \n35:16\nI'm sure you saw last year, there was a major  study where they had experienced developers try  \n35:21\nto close pull requests in repositories that they  were familiar with. Those developers reported an  \n35:28\nuplift. They reported that they felt more  productive with the use of these models.  But in fact, if you look at their output  and how much was actually merged back in,  \n35:35\nthere was a 20% downlift. They were less productive   as a result of using these models. So I'm trying to square the qualitative  \n35:40\nfeeling that people feel with these  models versus, 1) in a macro level,   where is this renaissance of software? And then 2) when people do these independent  \n35:48\nevaluations, why are we not seeing the  productivity benefits we would expect?  Within Anthropic, this is just really unambiguous. We're under an incredible amount of commercial  \n35:59\npressure and make it even harder for ourselves  because we have all this safety stuff we do that   I think we do more than other companies. The pressure to survive economically  \n36:11\nwhile also keeping our values is just incredible. We're trying to keep this 10x revenue curve going. \n36:18\nThere is zero time for bullshit. There is zero time for feeling   like we're productive when we're not. These tools make us a lot more productive. \n36:30\nWhy do you think we're concerned  about competitors using the tools?  Because we think we're ahead of the competitors. We wouldn't be going through all this trouble if  \n36:43\nthis were secretly reducing our productivity. We see the end productivity every few  \n36:49\nmonths in the form of model launches. There's no kidding yourself about this.  The models make you more productive. 1) People feeling like they're productive is  \n37:00\nqualitatively predicted by studies like this. But 2) if I just look at the end output,   obviously you guys are making fast progress. But the idea was supposed to be that with  \n37:10\nrecursive self-improvement, you make  a better AI, the AI helps you build a   better next AI, et cetera, et cetera. What I see instead\u2014if I look at you,  \n37:18\nOpenAI, DeepMind\u2014is that people are just  shifting around the podium every few months.  Maybe you think that stops  because you've won or whatever. \n37:25\nBut why are we not seeing the person with  the best coding model have this lasting  \n37:31\nadvantage if in fact there are these enormous  productivity gains from the last coding model. \n37:38\nI think my model of the situation is that  there's an advantage that's gradually growing. \n37:45\nI would say right now the coding  models give maybe, I don't know,  \n37:51\na 15-20% total factor speed up. That's  my view. Six months ago, it was maybe 5%. \n38:01\nSo it didn't matter. 5% doesn't register. It's now just getting to the point where it's  \n38:06\none of several factors that kind of matters. That's going to keep speeding up. \n38:12\nI think six months ago, there were several  companies that were at roughly the same  \n38:18\npoint because this wasn't a notable factor, but  I think it's starting to speed up more and more. \n38:25\nI would also say there are multiple companies that  write models that are used for code and we're not  \n38:32\nperfectly good at preventing some of these other  companies from using our models internally. \n38:41\nSo I think everything we're seeing is  consistent with this kind of snowball model. \n38:52\nAgain, my theme in all of this is all of this  is soft takeoff, soft, smooth exponentials,  \n39:00\nalthough the exponentials are relatively steep. So we're seeing this snowball gather momentum   where it's like 10%, 20%, 25%, 40%. As you go, Amdahl's law, you have  \n39:13\nto get all the things that are preventing  you from closing the loop out of the way.  But this is one of the biggest  priorities within Anthropic. \n39:22\nStepping back, before in the stack we were talking  about when do we get this on-the-job learning? \n39:29\nIt seems like the point you were making  on the coding thing is that we actually   don't need on-the-job learning. You can have tremendous productivity  \n39:36\nimprovements, you can have potentially trillions  of dollars of revenue for AI companies, without   this basic human ability to learn on the job. Maybe that's not your claim, you should clarify. \n39:47\nBut in most domains of economic activity, people  say, \"I hired somebody, they weren't that useful  \n39:53\nfor the first few months, and then over time  they built up the context, understanding.\"  It's actually hard to define  what we're talking about here. \n40:00\nBut they got something and then now they're  a powerhorse and they're so valuable to us. \n40:05\nIf AI doesn't develop this ability to learn on the  fly, I'm a bit skeptical that we're going to see  \n40:12\nhuge changes to the world without that ability. I think two things here. There's the state  \n40:17\nof the technology right now. Again, we have these two stages.  We have the pre-training and RL stage where  you throw a bunch of data and tasks into  \n40:27\nthe models and then they generalize. So it's like learning, but it's like   learning from more data and not learning  over one human or one model's lifetime. \n40:38\nSo again, this is situated between  evolution and human learning.  But once you learn all  those skills, you have them. \n40:45\nJust like with pre-training, just how the models  know more, if I look at a pre-trained model,  \n40:52\nit knows more about the history  of samurai in Japan than I do.  It knows more about baseball than I do. It knows more about low-pass filters  \n41:03\nand electronics, all of these things. Its knowledge is way broader than mine. \n41:08\nSo I think even just that may get us to the  point where the models are better at everything. \n41:18\nWe also have, again, just with scaling the kind  of existing setup, the in-context learning. \n41:24\nI would describe it as kind of  like human on-the-job learning,   but a little weaker and a little short term. You look at in-context learning and if you give  \n41:33\nthe model a bunch of examples it does get it. There's real learning that happens in context. \n41:38\nA million tokens is a lot. That can be days of human learning.  If you think about the model reading  a million words, how long would it  \n41:50\ntake me to read a million? Days or weeks  at least. So you have these two things. \n41:57\nI think these two things within the existing  paradigm may just be enough to get you the   \"country of geniuses in a data center\". I don't know for sure, but I think  \n42:04\nthey're going to get you a large fraction of it. There may be gaps, but I certainly think that just  \n42:10\nas things are, this is enough to generate  trillions of dollars of revenue. That's one. Two,  \n42:17\nis this idea of continual learning, this  idea of a single model learning on the job. \n42:24\nI think we're working on that too. There's a good chance that in the next   year or two, we also solve that. Again, I think you get most  \n42:36\nof the way there without it. The trillions of dollars a year market,  \n42:45\nmaybe all of the national security implications  and the safety implications that I wrote about in   \"Adolescence of Technology\" can happen without it. But we, and I imagine others, are working on it. \n42:57\nThere's a good chance that we will  get there within the next year or two. \n43:03\nThere are a bunch of ideas. I won't go into all of them in detail, but   one is just to make the context longer. There's nothing preventing  \n43:10\nlonger contexts from working. You just have to train at longer contexts   and then learn to serve them at inference. Both of those are engineering problems that  \n43:18\nwe are working on and I would assume  others are working on them as well.  This context length increase, it seemed  like there was a period from 2020 to 2023  \n43:26\nwhere from GPT-3 to GPT-4 Turbo, there was an  increase from 2000 context lengths to 128K. \n43:31\nI feel like for the two-ish years since  then, we've been in the same-ish ballpark. \n43:37\nWhen context lengths get much longer  than that, people report qualitative   degradation in the ability of the  model to consider that full context. \n43:47\nSo I'm curious what you're internally seeing  that makes you think, \"10 million contexts,   100 million contexts to get six months  of human learning and building context\". \n43:54\nThis isn't a research problem. This is  an engineering and inference problem.  If you want to serve long context, you  have to store your entire KV cache. \n44:06\nIt's difficult to store all the memory  in the GPUs, to juggle the memory around. \n44:11\nI don't even know the details. At this point, this is at a level of detail   that I'm no longer able to follow, although I  knew it in the GPT-3 era. \"These are the weights,  \n44:21\nthese are the activations you have to store\u2026\" But these days the whole thing is flipped  \n44:26\nbecause we have MoE models and all of that. Regarding this degradation you're talking about,  \n44:34\nwithout getting too specific, there's two things. There's the context length you train at and  \n44:41\nthere's a context length that you serve at. If you train at a small context length   and then try to serve at a long context  length, maybe you get these degradations. \n44:49\nIt's better than nothing, you might still  offer it, but you get these degradations.  Maybe it's harder to train  at a long context length. \n44:56\nI want to, at the same time, ask  about maybe some rabbit holes.  Wouldn't you expect that if you had  to train on longer context length,  \n45:04\nthat would mean that you're able to get less  samples in for the same amount of compute? \n45:10\nMaybe it's not worth diving deep on that. I want to get an answer to the   bigger picture question. I don't feel a preference  \n45:20\nfor a human editor that's been working for  me for six months versus an AI that's been   working with me for six months, what year  do you predict that that will be the case? \n45:33\nMy guess for that is there's a lot of problems  where basically we can do this when we have   the \"country of geniuses in a data center\". My picture for that, if you made me guess, is  \n45:48\none to two years, maybe one to three years. It's  really hard to tell. I have a strong view\u201499%,  \n45:54\n95%\u2014that all this will happen in 10 years. I think that's just a super safe bet. \n46:00\nI have a hunch\u2014this is more like a 50/50  thing\u2014that it's going to be more like   one to two, maybe more like one to three. So one to three years. Country of geniuses,  \n46:10\nand the slightly less economically  valuable task of editing videos.  It seems pretty economically  valuable, let me tell you. \n46:17\nIt's just there are a lot of use cases like that. There are a lot of similar ones.  So you're predicting that  within one to three years. \nIf AGI is imminent, why not buy more compute?\n46:23\nAnd then, generally, Anthropic has predicted that  by late '26 or early '27 we will have AI systems  \n46:28\nthat \"have the ability to navigate interfaces  available to humans doing digital work today,  \n46:34\nintellectual capabilities matching or exceeding  that of Nobel Prize winners, and the ability to   interface with the physical world\". You gave an interview two months ago  \n46:42\nwith DealBook where you were emphasizing  your company's more responsible compute  \n46:48\nscaling as compared to your competitors. I'm trying to square these two views.  If you really believe that we're going to  have a country of geniuses, you want as  \n46:57\nbig a data center as you can get. There's no reason to slow down.  The TAM of a Nobel Prize winner, that  can actually do everything a Nobel Prize  \n47:04\nwinner can do, is trillions of dollars. So I'm trying to square this conservatism,  \n47:10\nwhich seems rational if you have more moderate  timelines, with your stated views about progress. \n47:16\nIt actually all fits together. We go back to  this fast, but not infinitely fast, diffusion. \n47:23\nLet's say that we're making progress at this rate. The technology is making progress this fast. \n47:29\nI have very high conviction that we're  going to get there within a few years. \n47:39\nI have a hunch that we're going  to get there within a year or two.  So there\u2019s a little uncertainty on  the technical side, but pretty strong  \n47:46\nconfidence that it won't be off by much. What I'm less certain about is, again,   the economic diffusion side. I really do believe that we could  \n47:56\nhave models that are a country of geniuses  in the data center in one to two years. \n48:03\nOne question is: How many years after that  do the trillions in revenue start rolling in? \n48:14\nI don't think it's guaranteed  that it's going to be immediate. \n48:19\nIt could be one year, it could be two  years, I could even stretch it to five  \n48:27\nyears although I'm skeptical of that. So we have  this uncertainty. Even if the technology goes as  \n48:35\nfast as I suspect that it will, we don't know  exactly how fast it's going to drive revenue. \n48:41\nWe know it's coming, but with the way you buy  these data centers, if you're off by a couple  \n48:47\nyears, that can be ruinous. It is just like how I   wrote in \"Machines of Loving Grace\". I said I think we might get this powerful AI,  \n48:55\nthis \"country of genius in the data center\". That description you gave comes   from \"Machines of Loving Grace\". I said we'll get that in 2026, maybe 2027. Again,  \n49:02\nthat is my hunch. I wouldn't be surprised if  I'm off by a year or two, but that is my hunch.  \n49:08\nLet's say that happens. That's the starting gun.  How long does it take to cure all the diseases? \n49:13\nThat's one of the ways that drives a huge amount  of economic value. You cure every disease. There's  \n49:21\na question of how much of that goes to the  pharmaceutical company or the AI company,   but there's an enormous consumer surplus because  \u2014assuming we can get access for everyone,  \n49:29\nwhich I care about greatly\u2014we cure all of  these diseases. How long does it take? You   have to do the biological discovery,  you have to manufacture the new drug,  \n49:40\nyou have to go through the regulatory process. We saw this with vaccines and COVID. \n49:47\nWe got the vaccine out to everyone,  but it took a year and a half.  My question is: How long does it take to get  the cure for everything\u2014which AI is the genius  \n49:58\nthat can in theory invent\u2014out to everyone? How long from when that AI first exists   in the lab to when diseases have  actually been cured for everyone? \n50:09\nWe've had a polio vaccine for 50 years. We're still trying to eradicate it in the   most remote corners of Africa. The Gates Foundation is trying  \n50:18\nas hard as they can. Others are trying as hard   as they can. But that's difficult. Again, I  don't expect most of the economic diffusion  \n50:25\nto be as difficult as that. That's the most  difficult case. But there's a real dilemma here. \n50:32\nWhere I've settled on it is that it will  be faster than anything we've seen in the  \n50:39\nworld, but it still has its limits. So when we go to buying data centers,  \n50:47\nagain, the curve I'm looking at is: we've  had a 10x a year increase every year. \n50:54\nAt the beginning of this year, we're looking  at $10 billion in annualized revenue. \n51:02\nWe have to decide how much compute to buy. It takes a year or two to actually build out  \n51:10\nthe data centers, to reserve the data center. Basically I'm saying, \"In 2027,  \n51:16\nhow much compute do I get?\" I could assume that the  \n51:24\nrevenue will continue growing 10x a year,  so it'll be $100 billion at the end of  \n51:31\n2026 and $1 trillion at the end of 2027. Actually it would be $5 trillion dollars  \n51:39\nof compute because it would be $1  trillion a year for five years.  I could buy $1 trillion of compute  that starts at the end of 2027. \n51:49\nIf my revenue is not $1 trillion dollars, if it's  even $800 billion, there's no force on earth,  \n51:56\nthere's no hedge on earth that could stop me  from going bankrupt if I buy that much compute. \n52:03\nEven though a part of my brain wonders  if it's going to keep growing 10x,   I can't buy $1 trillion a year of compute in 2027. If I'm just off by a year in that rate of growth,  \n52:17\nor if the growth rate is 5x a year instead  of 10x a year, then you go bankrupt. \n52:25\nSo you end up in a world where you're  supporting hundreds of billions, not trillions. \n52:33\nYou accept some risk that there's so much  demand that you can't support the revenue,  \n52:38\nand you accept some risk that you  got it wrong and it's still slow. \n52:43\nWhen I talked about behaving responsibly, what  I meant actually was not the absolute amount. \n52:51\nI think it is true we're spending somewhat  less than some of the other players.  It's actually the other things, like have we been  thoughtful about it or are we YOLOing and saying,  \n53:01\n\"We're going to do $100 billion  here or $100 billion there\"?  I get the impression that some of the  other companies have not written down  \n53:09\nthe spreadsheet, that they don't really  understand the risks they're taking.  They're just doing stuff because it sounds  cool. We've thought carefully about it. We're  \n53:19\nan enterprise business. Therefore, we can rely  more on revenue. It's less fickle than consumer.  \n53:26\nWe have better margins, which is the buffer  between buying too much and buying too little. \n53:31\nI think we bought an amount that allows  us to capture pretty strong upside worlds. \n53:37\nIt won't capture the full 10x a year. Things would have to go pretty badly for  \n53:42\nus to be in financial trouble. So we've thought carefully and   we've made that balance. That's what I mean when  \n53:48\nI say that we're being responsible. So it seems like it's possible that we  \n53:54\nactually just have different definitions of  the \"country of a genius in a data center\".  Because when I think of actual human geniuses, an  actual country of human geniuses in a data center,  \n54:02\nI would happily buy $5 trillion worth  of compute to run an actual country of  \n54:08\nhuman geniuses in a data center. Let's say JPMorgan or Moderna or   whatever doesn't want to use them. I've got a country of geniuses.  \n54:14\nThey'll start their own company. If they can't  start their own company and they're bottlenecked   by clinical trials\u2026 It is worth stating that with  clinical trials, most clinical trials fail because  \n54:22\nthe drug doesn't work. There's not efficacy. I make exactly that point in \"Machines of   Loving Grace\", I say the clinical  trials are going to go much faster  \n54:30\nthan we're used to, but not infinitely fast. Okay, and then suppose it takes a year for  \n54:35\nthe clinical trials to work out so that you're  getting revenue from that and can make more drugs.  Okay, well, you've got a country  of geniuses and you're an AI lab. \n54:44\nYou could use many more AI researchers. You also think there are these self-reinforcing  \n54:50\ngains from smart people working on AI tech. You can have the data center  \n54:56\nworking on AI progress. Are there substantially  \n55:01\nmore gains from buying $1 trillion a year of  compute versus $300 billion a year of compute? \n55:07\nIf your competitor is buying  a trillion, yes there is.  Well, no, there's some gain, but then again,  there's this chance that they go bankrupt before. \n55:17\nAgain, if you're off by only a year, you  destroy yourselves. That's the balance. We're  \n55:23\nbuying a lot. We're buying a hell of a lot. We're buying an amount that's comparable to  \n55:30\nwhat the biggest players in the game are buying. But if you're asking me, \"Why haven't we signed  \n55:39\n$10 trillion of compute starting in mid-2027?\"... First of all, it can't be produced. \n55:44\nThere isn't that much in the world. But second, what if the country of  \n55:50\ngeniuses comes, but it comes in mid-2028  instead of mid-2027? You go bankrupt. \n55:56\nSo if your projection is one to three  years, it seems like you should want   $10 trillion of compute by 2029 at the latest? Even in the longest version of the timelines  \n56:11\nyou state, the compute you are ramping  up to build doesn't seem in accordance.  What makes you think that? Human wages, let's say,  \n56:21\nare on the order of $50 trillion a year\u2014 So I won't talk about Anthropic in particular,  \n56:27\nbut if you talk about the industry, the amount  of compute the industry is building this year is  \n56:38\nprobably, call it, 10-15 gigawatts. It goes up by roughly 3x a year. \n56:48\nSo next year's 30-40 gigawatts. 2028 might be  100 gigawatts. 2029 might be like 300 gigawatts. \n57:03\nI'm doing the math in my head, but  each gigawatt costs maybe $10 billion,   on the order of $10-15 billion a year. You put that all together and you're  \n57:14\ngetting about what you described. You\u2019re  getting exactly that. You're getting multiple   trillions a year by 2028 or 2029. You're getting exactly what you predict. \n57:23\nThat's for the industry. That's for the industry, that\u2019s right.  Suppose Anthropic's compute keeps 3x-ing a year,  and then by 2027-28, you have 10 gigawatts. \n57:34\nMultiply that by, as you say, $10 billion. So then it's like $100 billion a year. \n57:40\nBut then you're saying the  TAM by 2028 is $200 billion.  Again, I don't want to give exact numbers for  Anthropic, but these numbers are too small. \n57:48\nOkay, interesting. You've told investors  \nHow will AI labs actually make profit?\n58:49\nthat you plan to be profitable starting in 2028. This is the year when we're potentially getting  \n58:55\nthe country of geniuses as a data center. This is now going to unlock all this progress  \n59:02\nin medicine and health and new technologies. Wouldn't this be exactly the time where you'd  \n59:11\nwant to reinvest in the business and build bigger  \"countries\" so they can make more discoveries? \n59:16\nProfitability is this kind  of weird thing in this field. \n59:21\nI don't think in this field profitability  is actually a measure of spending down  \n59:32\nversus investing in the business. Let's just take a model of this.  I actually think profitability happens when you  underestimated the amount of demand you were going  \n59:41\nto get and loss happens when you overestimated  the amount of demand you were going to get,  \n59:46\nbecause you're buying the data centers ahead  of time. Think about it this way. Again,  \n59:52\nthese are stylized facts. These numbers are not  exact. I'm just trying to make a toy model here.  Let's say half of your compute is for training  and half of your compute is for inference. \n1:00:02\nThe inference has some gross  margin that's more than 50%.  So what that means is that if you were in  steady-state, you build a data center and if  \n1:00:12\nyou knew exactly the demand you were getting,  you would get a certain amount of revenue. \n1:00:23\nLet\u2019s say you pay $100 billion a year for compute. On $50 billion a year you support  \n1:00:28\n$150 billion of revenue. The other $50 billion is used for training. \n1:00:36\nBasically you\u2019re profitable and  you make $50 billion of profit.  Those are the economics of the industry  today, or not today but where we\u2019re  \n1:00:45\nprojecting forward in a year or two. The only thing that makes that not the   case is if you get less demand than $50 billion. Then you have more than 50% of your data center  \n1:00:57\nfor research and you're not profitable. So you train stronger models,   but you're not profitable. If you get more demand than you thought, then  \n1:01:07\nresearch gets squeezed, but you're kind of able to  support more inference and you're more profitable. \n1:01:16\nMaybe I'm not explaining it well, but  the thing I'm trying to say is that you   decide the amount of compute first. Then you have some target desire of  \n1:01:24\ninference versus training, but  that gets determined by demand.  It doesn't get determined by you. What I'm hearing is the reason  \n1:01:30\nyou're predicting profit is that you are  systematically underinvesting in compute? \n1:01:37\nNo, no, no. I'm saying it's hard to predict. These things about 2028 and when it will happen,  \n1:01:43\nthat's our attempt to do the  best we can with investors.  All of this stuff is really uncertain  because of the cone of uncertainty. \n1:01:50\nWe could be profitable in 2026  if the revenue grows fast enough. \n1:01:58\nIf we overestimate or underestimate  the next year, that could swing wildly. \n1:02:04\nWhat I'm trying to get at is that you have a  model in your head of a business that invests,  \n1:02:09\ninvests, invests, gets scale  and then becomes profitable.  There's a single point at  which things turn around. \n1:02:16\nI don't think the economics of  this industry work that way.  I see. So if I'm understanding correctly,  you're saying that because of the discrepancy  \n1:02:24\nbetween the amount of compute we should have  gotten and the amount of compute we got,   we were sort of forced to make profit. But that doesn't mean we're going  \n1:02:30\nto continue making profit. We're going to reinvest the money   because now AI has made so much progress  and we want a bigger country of geniuses. \n1:02:37\nSo back into revenue is high,  but losses are also high. \n1:02:44\nIf every year we predict exactly what the demand  is going to be, we'll be profitable every year. \n1:02:50\nBecause spending 50% of your compute on research,  roughly, plus a gross margin that's higher than  \n1:03:00\n50% and correct demand prediction leads to profit. That's the profitable business model that I think  \n1:03:07\nis kind of there, but obscured by these  building ahead and prediction errors. \n1:03:13\nI guess you're treating the 50% as a  sort of given constant, whereas in fact,  \n1:03:21\nif AI progress is fast and you can increase the  progress by scaling up more, you should just have   more than 50% and not make profit. But here's what I'll say. You  \n1:03:26\nmight want to scale it up more. Remember the log returns to scale. \n1:03:34\nIf 70% would get you a very little bit of  a smaller model through a factor of 1.4x... \n1:03:42\nThat extra $20 billion, each dollar there is worth  much less to you because of the log-linear setup. \n1:03:51\nSo you might find that it's better  to invest that $20 billion in serving  \n1:03:58\ninference or in hiring engineers who are  kind of better at what they're doing. \n1:04:05\nSo the reason I said 50%... That's not exactly  our target. It's not exactly going to be 50%.  \n1:04:10\nIt\u2019ll probably vary over time. What I'm saying  is the log-linear return, what it leads to is you  \n1:04:18\nspend of order one fraction of the business. Like  not 5%, not 95%. Then you get diminishing returns. \n1:04:28\nI feel strange that I'm convincing Dario  to believe in AI progress or something. \n1:04:34\nOkay, you don't invest in research  because it has diminishing returns,   but you invest in the other things you mentioned. I think profit at a sort of macro level\u2014  Again, I'm talking about diminishing returns,  but after you're spending $50 billion a year. \n1:04:46\nThis is a point I'm sure you would make,  but diminishing returns on a genius could  \n1:04:51\nbe quite high. More generally,   what is profit in a market economy? Profit is basically saying other  \n1:04:58\ncompanies in the market can do more  things with this money than I can.  Put aside Anthropic. I don't want  to give information about Anthropic. \n1:05:06\nThat\u2019s why I'm giving these stylized numbers. But let's just derive the   equilibrium of the industry. Why doesn't everyone spend 100% of their  \n1:05:21\ncompute on training and not serve any customers? It's because if they didn't get any revenue,   they couldn't raise money,  they couldn't do compute deals,  \n1:05:27\nthey couldn't buy more compute the next year. So there's going to be an equilibrium where every   company spends less than 100% on training  and certainly less than 100% on inference. \n1:05:38\nIt should be clear why you don't just serve the  current models and never train another model,  \n1:05:44\nbecause then you don't have any demand because  you'll fall behind. So there's some equilibrium.   It's not gonna be 10%, it's not gonna be 90%. Let's just say as a stylized fact, it's 50%.  \n1:05:55\nThat's what I'm getting at. I think we're gonna be  in a position where that equilibrium of how much  \n1:06:01\nyou spend on training is less than the gross  margins that you're able to get on compute. \n1:06:08\nSo the underlying economics are profitable. The problem is you have this hellish demand  \n1:06:14\nprediction problem when you're buying the next  year of compute and you might guess under and be  \n1:06:21\nvery profitable but have no compute for research. Or you might guess over and you are not  \n1:06:30\nprofitable and you have all the compute for  research in the world. Does that make sense?  \n1:06:36\nJust as a dynamic model of the industry? Maybe stepping back, I'm not saying I think  \n1:06:42\nthe \"country of geniuses\" is going to come in two  years and therefore you should buy this compute.  To me, the end conclusion you're  arriving at makes a lot of sense. \n1:06:51\nBut that's because it seems like \"country of  geniuses\" is hard and there's a long way to go. \n1:06:57\nSo stepping back, the thing I'm trying to get  at is more that it seems like your worldview  \n1:07:03\nis compatible with somebody who says, \"We're  like 10 years away from a world in which we're   generating trillions of dollars of value.\" That's just not my view. So I'll make  \n1:07:14\nanother prediction. It is hard for me  to see that there won't be trillions  \n1:07:20\nof dollars in revenue before 2030. I can construct a plausible world.  \n1:07:26\nIt takes maybe three years. That would be  the end of what I think it's plausible. \n1:07:31\nLike in 2028, we get the real \"country  of geniuses in the data center\". \n1:07:36\nThe revenue's going into the low hundreds  of billions by 2028, and then the country  \n1:07:46\nof geniuses accelerates it to trillions. We\u2019re basically on the slow end of diffusion. \n1:07:52\nIt takes two years to get to the trillions. That would be the world where it takes until 2030. \n1:07:59\nI suspect even composing the technical  exponential and diffusion exponential,  \n1:08:05\nwe\u2019ll get there before 2030. So you laid out a model where Anthropic makes  \n1:08:10\nprofit because it seems like fundamentally  we're in a compute-constrained world.  So eventually we keep growing compute\u2014 I think the way the profit comes is\u2026 Again,  \n1:08:21\nlet's just abstract the whole industry here. Let's just imagine we're in an economics textbook. \n1:08:27\nWe have a small number of firms. Each can invest a limited amount. \n1:08:33\nEach can invest some fraction in R&D. They have some marginal cost to serve.  The gross profit margins on that marginal cost  are very high because inference is efficient. \n1:08:47\nThere's some competition, but the  models are also differentiated. \n1:08:52\nCompanies will compete to push  their research budgets up.  But because there's a small number of  players, we have the... What is it called?  \n1:09:00\nThe Cournot equilibrium, I think, is what  the small number of firm equilibrium is. \n1:09:05\nThe point is it doesn't equilibrate to  perfect competition with zero margins. \n1:09:15\nIf there's three firms in the economy and all  are kind of independently behaving rationally,  \n1:09:20\nit doesn't equilibrate to zero. Help me understand that, because   right now we do have three leading firms and  they're not making profit. So what is changing? \n1:09:33\nAgain, the gross margins  right now are very positive.  What's happening is a combination of two things. One is that we're still in the exponential  \n1:09:43\nscale-up phase of compute. A model  gets trained. Let's say a model got  \n1:09:53\ntrained that costs $1 billion last year. Then this year it produced $4 billion of  \n1:10:02\nrevenue and cost $1 billion to inference from. Again, I'm using stylized numbers here, but that  \n1:10:12\nwould be 75% gross margins and this 25% tax. So that model as a whole makes $2 billion. \n1:10:23\nBut at the same time, we're spending $10  billion to train the next model because   there's an exponential scale-up. So  the company loses money. Each model  \n1:10:31\nmakes money, but the company loses money. The equilibrium I'm talking about is an   equilibrium where we have the \"country  of geniuses in a data center\", but that  \n1:10:43\nmodel training scale-up has equilibrated more.  Maybe it's still going up. We're still trying to  \n1:10:49\npredict the demand, but it's more leveled out. I'm confused about a couple of things there. \n1:10:56\nLet's start with the current world. In the current world, you're right that,   as you said before, if you treat each  individual model as a company, it's profitable. \n1:11:05\nBut of course, a big part of the production  function of being a frontier lab is training  \n1:11:11\nthe next model, right? Yes, that's right.  If you didn't do that, then you'd  make profit for two months and then   you wouldn't have margins because  you wouldn't have the best model. \n1:11:19\nBut at some point that reaches the  biggest scale that it can reach.  And then in equilibrium, we have algorithmic  improvements, but we're spending roughly the  \n1:11:28\nsame amount to train the next model as  we spend to train the current model. \n1:11:37\nAt some point you run out of money in the economy. A fixed lump of labor fallacy\u2026 The economy is  \n1:11:42\ngoing to grow, right? That's one  of your predictions. We're going   to have the data centers in space. Yes, but this is another example   of the theme I was talking about. The economy will grow much faster  \n1:11:53\nwith AI than I think it ever has before. Right now the compute is growing 3x a year. \n1:11:59\nI don't believe the economy  is gonna grow 300% a year.  I said this in \"Machines of Loving  Grace\", I think we may get 10-20%  \n1:12:08\nper year growth in the economy, but we're  not gonna get 300% growth in the economy. \n1:12:13\nSo I think in the end, if compute becomes  the majority of what the economy produces,  \n1:12:18\nit's gonna be capped by that. So let's assume a model   where compute stays capped. The world where frontier labs are making money  \n1:12:26\nis one where they continue to make fast progress. Because fundamentally your margin is limited by  \n1:12:34\nhow good the alternative is. So you are able to make money   because you have a frontier model. If you didn't have a frontier model  \n1:12:39\nyou wouldn't be making money. So this model requires there  \n1:12:45\nnever to be a steady state. Forever and ever you keep   making more algorithmic progress. I don't think that's true. I mean,  \n1:12:51\nI feel like we're in an economics class. Do you know the Tyler Cowen quote? \n1:12:59\nWe never stop talking about economics. We never stop talking about economics.  So no, I don't think this  field's going to be a monopoly. \n1:13:12\nAll my lawyers never want me  to say the word \"monopoly\".  But I don't think this field's  going to be a monopoly. \n1:13:17\nYou do get industries in which  there are a small number of players.  Not one, but a small number of players. Ordinarily, the way you get monopolies  \n1:13:27\nlike Facebook or Meta\u2014I always call them  Facebook\u2014is these kinds of network effects. \n1:13:37\nThe way you get industries in which  there are a small number of players,   is very high costs of entry. Cloud is like  this. I think cloud is a good example of this. \n1:13:49\nThere are three, maybe four, players within cloud. I think that's the same for AI, three, maybe four. \n1:13:56\nThe reason is that it's so expensive. It requires so much expertise and so  \n1:14:02\nmuch capital to run a cloud company. You have to put up all this capital. \n1:14:08\nIn addition to putting up all this capital,  you have to get all of this other stuff   that requires a lot of skill to make it happen. So if you go to someone and you're like, \"I want  \n1:14:17\nto disrupt this industry, here's $100 billion.\" You're like, \"okay, I'm putting in $100 billion  \n1:14:22\nand also betting that you can do all these  other things that these people have been doing.\"  Only to decrease the profit. The effect of your entering  \n1:14:29\nis that profit margins go down. So, we have equilibria like this   all the time in the economy where we have a few  players. Profits are not astronomical. Margins  \n1:14:39\nare not astronomical, but they're not zero. That's what we see on cloud. Cloud is very  \n1:14:47\nundifferentiated. Models are  more differentiated than cloud.  Everyone knows Claude is good at different things  than GPT is good at, than Gemini is good at. \n1:14:58\nIt's not just that Claude's good at  coding, GPT is good at math and reasoning.  \n1:15:05\nIt's more subtle than that. Models are good at  different types of coding. Models have different   styles. I think these things are actually quite  different from each other, and so I would expect  \n1:15:15\nmore differentiation than you see in cloud. Now, there actually is one counter-argument. \n1:15:26\nThat counter-argument is if the  process of producing models,  \n1:15:32\nif AI models can do that themselves, then  that could spread throughout the economy.  But that is not an argument for  commoditizing AI models in general. \n1:15:41\nThat's kind of an argument for  commoditizing the whole economy at once.  I don't know what quite happens in  that world where basically anyone  \n1:15:48\ncan do anything, anyone can build anything,  and there's no moat around anything at all.  I don't know, maybe we want that world. Maybe that's the end state here. \n1:15:58\nMaybe when AI models can do everything, if we've  solved all the safety and security problems,  \n1:16:09\nthat's one of the mechanisms for the  economy just flattening itself again. \n1:16:17\nBut that's kind of far post-\"country  of geniuses in the data center.\" \n1:16:23\nMaybe a finer way to put that potential point  is: 1) it seems like AI research is especially  \n1:16:32\nloaded on raw intellectual power, which will  be especially abundant in the world of AGI. \n1:16:37\nAnd 2) if you just look at the world today,  there are very few technologies that seem to be   diffusing as fast as AI algorithmic progress. So that does hint that this industry is  \n1:16:50\nsort of structurally diffusive. I think coding is going fast, but   I think AI research is a superset of coding and  there are aspects of it that are not going fast. \n1:17:00\nBut I do think, again, once we get coding, once we  get AI models going fast, then that will speed up  \n1:17:07\nthe ability of AI models to do everything else. So while coding is going fast now, I think once  \n1:17:13\nthe AI models are building the next AI  models and building everything else,   the whole economy will kind of go at the same  pace. I am worried geographically, though.  \n1:17:24\nI'm a little worried that just proximity to AI,  having heard about AI, may be one differentiator. \n1:17:34\nSo when I said the 10-20% growth rate, a worry  I have is that the growth rate could be like 50%  \n1:17:42\nin Silicon Valley and parts of the world that are  socially connected to Silicon Valley, and not that  \n1:17:50\nmuch faster than its current pace elsewhere. I think that'd be a pretty messed up world.  So one of the things I think about  a lot is how to prevent that. \n1:17:57\nDo you think that once we have this  country of geniuses in a data center, that   robotics is sort of quickly solved afterwards? Because it seems like a big problem with robotics  \n1:18:06\nis that a human can learn how to teleoperate  current hardware, but current AI models can't,  \n1:18:12\nat least not in a way that's super productive. And so if we have this ability to learn like   a human, shouldn't it solve  robotics immediately as well? \n1:18:19\nI don't think it's dependent  on learning like a human.  It could happen in different ways. Again, we could have trained the model on  \n1:18:25\nmany different video games, which are like robotic  controls, or many different simulated robotics   environments, or just train them to control  computer screens, and they learn to generalize. \n1:18:34\nSo it will happen... it's not necessarily  dependent on human-like learning. \n1:18:41\nHuman-like learning is one way it could happen. If the model's like, \"Oh, I pick up a robot,   I don't know how to use it, I learn,\" that could  happen because we discovered continual learning. \n1:18:50\nThat could also happen because we trained  the model on a bunch of environments and   then generalized, or it could happen because  the model learns that in the context length. \n1:18:58\nIt doesn't actually matter which way. If we go back to the discussion we had   an hour ago, that type of thing can  happen in several different ways. \n1:19:10\nBut I do think when for whatever reason the  models have those skills, then robotics will be  \n1:19:16\nrevolutionized\u2014both the design of robots, because  the models will be much better than humans at   that, and also the ability to control robots. So we'll get better at building the physical  \n1:19:28\nhardware, building the physical robots, and  we'll also get better at controlling it.  Now, does that mean the robotics  industry will also be generating  \n1:19:36\ntrillions of dollars of revenue? My answer there is yes, but there will be   the same extremely fast, but not infinitely fast  diffusion. So will robotics be revolutionized?  \n1:19:46\nYeah, maybe tack on another year or two. That's the way I think about these things. \n1:19:52\nMakes sense. There's a general skepticism about  extremely fast progress. Here's my view. It sounds  \n1:19:58\nlike you are going to solve continual learning  one way or another within a matter of years.  But just as people weren't talking about  continual learning a couple of years ago,  \n1:20:06\nand then we realized, \"Oh, why aren't these  models as useful as they could be right now,   even though they are clearly passing the Turing  test and are experts in so many different domains?  \n1:20:14\nMaybe it's this thing.\" Then we solve this thing  and we realize, actually, there's another thing  \n1:20:19\nthat human intelligence can do that's a basis  of human labor that these models can't do.  So why not think there will be  more things like this, where  \n1:20:28\nwe've found more pieces of human intelligence? Well, to be clear, I think continual learning, as  \n1:20:33\nI've said before, might not be a barrier at all. I think we may just get there by pre-training  \n1:20:40\ngeneralization and RL generalization. I think there just  \n1:20:48\nmight not be such a thing at all. In fact, I would point to the history   in ML of people coming up with things  that are barriers that end up kind of  \n1:20:56\ndissolving within the big blob of compute. People talked about, \"How do your models  \n1:21:06\nkeep track of nouns and verbs?\"  \"They can understand syntactically,  \n1:21:11\nbut they can't understand semantically? It's only statistical correlations.\"   \"You can understand a paragraph,  you can\u2019t understand a word. \n1:21:19\nThere's reasoning, you can't do reasoning.\" But then suddenly it turns out you can   do code and math very well. So I think there's actually a  \n1:21:27\nstronger history of some of these things seeming  like a big deal and then kind of dissolving. Some  \n1:21:35\nof them are real. The need for data is real,  maybe continual learning is a real thing. \n1:21:42\nBut again, I would ground  us in something like code.  I think we may get to the point in  a year or two where the models can  \n1:21:50\njust do SWE end-to-end. That's a whole task.  That's a whole sphere of human activity that  \n1:21:56\nwe're just saying models can do now. When you say end-to-end, do you mean  \n1:22:02\nsetting technical direction, understanding  the context of the problem, et cetera?  Yes. I mean all of that. Interesting. I feel like that is AGI-complete,  \n1:22:13\nwhich maybe is internally consistent. But it's not like saying 90%   of code or 100% of code. No, I gave this spectrum:  \n1:22:22\n90% of code, 100% of code, 90% of  end-to-end SWE, 100% of end-to-end SWE. \n1:22:28\nNew tasks are created for SWEs. Eventually those get done as well.  It's a long spectrum there, but we're  traversing the spectrum very quickly. \n1:22:35\nI do think it's funny that I've seen  a couple of podcasts you've done where   the hosts will be like, \"But Dwarkesh wrote  the essay about the continuous learning thing.\" \n1:22:43\nIt always makes me crack up because  you've been an AI researcher for 10 years. \n1:22:48\nI'm sure there's some feeling of,  \"Okay, so a podcaster wrote an essay,   and every interview I get asked about it.\" The truth of the matter is that we're all  \n1:22:59\ntrying to figure this out together. There are some ways in which I'm  \n1:23:04\nable to see things that others aren't. These days that probably has more to do   with seeing a bunch of stuff within Anthropic and  having to make a bunch of decisions than I have  \n1:23:13\nany great research insight that others don't. I'm running a 2,500 person company. \n1:23:20\nIt's actually pretty hard for me to have concrete  research insight, much harder than it would have  \n1:23:27\nbeen 10 years ago or even two or three years ago. As we go towards a world of a full drop-in  \n1:23:36\nremote worker replacement, does an API  pricing model still make the most sense? \n1:23:42\nIf not, what is the correct  way to price AGI, or serve AGI?  I think there's going to be a bunch of  different business models here, all at once,  \n1:23:49\nthat are going to be experimented with. I actually do think that the API  \n1:23:59\nmodel is more durable than many people think. One way I think about it is if the technology  \n1:24:06\nis advancing quickly, if it's advancing  exponentially, what that means is there's  \n1:24:12\nalways a surface area of new use cases that  have been developed in the last three months. \n1:24:20\nAny kind of product surface you put in place is  always at risk of sort of becoming irrelevant. \n1:24:27\nAny given product surface probably makes sense  for a range of capabilities of the model. \n1:24:32\nThe chatbot is already running into limitations  where making it smarter doesn't really help the  \n1:24:39\naverage consumer that much. But I don't think that's   a limitation of AI models. I don't think that's evidence  \n1:24:45\nthat the models are good enough and them  getting better doesn't matter to the economy. \n1:24:51\nIt doesn't matter to that particular product. So I think the value of the API is that the API  \n1:24:58\nalways offers an opportunity, very close to the  bare metal, to build on what the latest thing is. \n1:25:06\nThere's always going to be this front  of new startups and new ideas that  \n1:25:14\nweren't possible a few months ago and are  possible because the model is advancing. \n1:25:19\nI actually predict that it's going to exist  alongside other models, but we're always going  \n1:25:28\nto have the API business model because there's  always going to be a need for a thousand different  \n1:25:34\npeople to try experimenting with the model in a  different way. 100 of them become startups and  \n1:25:40\nten of them become big successful startups. Two or three really end up being the way   that people use the model of a given generation. So I basically think it's always going to exist. \n1:25:50\nAt the same time, I'm sure there's  going to be other models as well.  Not every token that's output by  the model is worth the same amount. \n1:26:00\nThink about what is the value of the tokens  that the model outputs when someone calls  \n1:26:10\nthem up and says, \"My Mac isn't working,\" or  something, the model's like, \"restart it.\" \n1:26:16\nSomeone hasn't heard that before, but  the model said that 10 million times. \n1:26:23\nMaybe that's worth like a dollar  or a few cents or something.  Whereas if the model goes to one of the  pharmaceutical companies and it says, \"Oh,  \n1:26:34\nyou know, this molecule you're developing, you  should take the aromatic ring from that end of the   molecule and put it on that end of the molecule. If you do that, wonderful things will happen.\" \n1:26:46\nThose tokens could be worth  tens of millions of dollars. \n1:26:52\nSo I think we're definitely going to  see business models that recognize that.  At some point we're going to see \"pay for results\"  in some form, or we may see forms of compensation  \n1:27:06\nthat are like labor, that kind of work by the  hour. I don't know. I think because it's a new  \n1:27:16\nindustry, a lot of things are going to be tried. I don't know what will turn out to   be the right thing. I take your point that  \n1:27:24\npeople will have to try things to figure out what  is the best way to use this blob of intelligence.  But what I find striking is Claude Code. I don't think in the history of startups  \n1:27:34\nthere has been a single application that has  been as hotly competed in as coding agents. \n1:27:42\nClaude Code is a category leader here. That  seems surprising to me. It doesn't seem  \n1:27:49\nintrinsically that Anthropic had to build this. I wonder if you have an accounting of why it had   to be Anthropic or how Anthropic ended  up building an application in addition  \n1:27:58\nto the model underlying it that was successful. So it actually happened in a pretty simple way,   which is that we had our own coding  models, which were good at coding. \n1:28:09\nAround the beginning of 2025, I said, \"I  think the time has come where you can have   nontrivial acceleration of your own research  if you're an AI company by using these models.\" \n1:28:21\nOf course, you need an interface,  you need a harness to use them.   So I encouraged people internally. I didn't  say this is one thing that you have to use. \n1:28:31\nI just said people should experiment with this. I think it might have been originally  \n1:28:37\ncalled Claude CLI, and then the name  eventually got changed to Claude Code.  Internally, it was the thing that everyone was  using and it was seeing fast internal adoption. \n1:28:48\nI looked at it and I said, \"Probably we  should launch this externally, right?\"  It's seen such fast adoption within Anthropic. Coding is a lot of what we do. \n1:28:59\nWe have an audience of many, many hundreds  of people that's in some ways at least   representative of the external audience. So it looks like we already have product  \n1:29:08\nmarket fit. Let's launch this thing. And then  we launched it. I think just the fact that we  \n1:29:15\nourselves are kind of developing the model and we  ourselves know what we most need to use the model,  \n1:29:21\nI think it's kind of creating this feedback loop. I see. In the sense that you, let's say a  \n1:29:26\ndeveloper at Anthropic is like, \"Ah, it would  be better if it was better at this X thing.\" \n1:29:31\nThen you bake that into the  next model that you build.  That's one version of it, but then there's  just the ordinary product iteration. \n1:29:41\nWe have a bunch of coders within  Anthropic, they use Claude Code  \n1:29:47\nevery day and so we get fast feedback. That was more important in the early days.  Now, of course, there are millions  of people using it, and so we get  \n1:29:53\na bunch of external feedback as well. But it's just great to be able to get   kind of fast internal feedback. I think this is the reason why we  \n1:30:03\nlaunched a coding model and didn't  launch a pharmaceutical company. \n1:30:10\nMy background's in biology, but we  don't have any of the resources that   are needed to launch a pharmaceutical company. Let me now ask you about making AI go well. \nWill regulations destroy the boons of AGI?\n1:31:24\nIt seems like whatever vision we have about how  AI goes well has to be compatible with two things:  \n1:31:30\n1) the ability to build and run AIs is  diffusing extremely rapidly and 2) the  \n1:31:37\npopulation of AIs, the amount we have and their  intelligence, will also increase very rapidly. \n1:31:44\nThat means that lots of people will be able  to build huge populations of misaligned AIs,  \n1:31:49\nor AIs which are just companies  which are trying to increase their   footprint or have weird psyches like  Sydney Bing, but now they're superhuman. \n1:31:57\nWhat is a vision for a world in which we  have an equilibrium that is compatible   with lots of different AIs, some of  which are misaligned, running around? \n1:32:06\nI think in \"The Adolescence of Technology\",  I was skeptical of the balance of power. \n1:32:13\nBut the thing I was specifically skeptical of  is you have three or four of these companies  \n1:32:23\nall building models that are derived from the  same thing, that they would check each other. \n1:32:36\nOr even that any number of  them would check each other.  We might live in an offense-dominant world where  one person or one AI model is smart enough to do  \n1:32:47\nsomething that causes damage for everything else. In the short run, we have a limited number  \n1:32:54\nof players now. So we can start   within the limited number of players. We need to put in place the safeguards. \n1:33:03\nWe need to make sure everyone  does the right alignment work.  We need to make sure everyone has bioclassifiers. Those are the immediate things we need to do. \n1:33:11\nI agree that that doesn't solve the problem in  the long run, particularly if the ability of  \n1:33:16\nAI models to make other AI models proliferates,  then the whole thing can become harder to solve. \n1:33:26\nI think in the long run we need  some architecture of governance.  We need some architecture of governance  that preserves human freedom,  \n1:33:35\nbut also allows us to govern a very large  number of human systems, AI systems, hybrid  \n1:33:52\nhuman-AI companies or economic units. So we're gonna need to think about:  \n1:34:01\nhow do we protect the world against bioterrorism? How do we protect the world against mirror life? \n1:34:11\nProbably we're gonna need some  kind of AI monitoring system   that monitors for all of these things. But then we need to build this in a way  \n1:34:20\nthat preserves civil liberties  and our constitutional rights.  So I think just as anything else, it's a  new security landscape with a new set of  \n1:34:34\ntools and a new set of vulnerabilities. My worry is, if we had 100 years for this  \n1:34:40\nto happen all very slowly, we'd get used to it. We've gotten used to the presence of explosives  \n1:34:49\nin society or the presence of various new  weapons or the presence of video cameras. \n1:34:58\nWe would get used to it over 100 years and  we\u2019d develop governance mechanisms. We'd   make our mistakes. My worry is just  that this is happening all so fast. \n1:35:07\nSo maybe we need to do our thinking faster about  how to make these governance mechanisms work. \n1:35:13\nIt seems like in an offense-dominant world, over  the course of the next century\u2014the idea is that AI  \n1:35:19\nis making the progress that would happen over the  next century happen in some period of five to ten   years\u2014we would still need the same mechanisms, or  balance of power would be similarly intractable,  \n1:35:29\neven if humans were the only game in town. I guess we have the advice of AI. \n1:35:36\nBut it fundamentally doesn't seem like  a totally different ball game here. \n1:35:41\nIf checks and balances were going to  work, they would work with humans as well.  If they aren't going to work, they  wouldn't work with AIs as well. \n1:35:47\nSo maybe this just dooms human  checks and balances as well.  Again, I think there's some  way to make this happen. \n1:35:58\nThe governments of the world may have  to work together to make it happen.  We may have to talk to AIs about building  societal structures in such a way that these  \n1:36:10\ndefenses are possible. I don't know. I don\u2019t  want to say this is so far ahead in time,  \n1:36:15\nbut it\u2019s so far ahead in technological ability  that may happen over a short period of time,  \n1:36:21\nthat it's hard for us to anticipate it in advance. Speaking of governments getting involved,   on December 26, the Tennessee legislature  introduced a bill which said, \"It would  \n1:36:31\nbe an offense for a person to knowingly  train artificial intelligence to provide   emotional support, including through  open-ended conversations with a user.\" \n1:36:39\nOf course, one of the things that Claude attempts  to do is be a thoughtful, knowledgeable friend. \n1:36:48\nIn general, it seems like we're going  to have this patchwork of state laws.  A lot of the benefits that normal people could  experience as a result of AI are going to be  \n1:36:56\ncurtailed, especially when we get into the  kinds of things you discuss in \"Machines   of Loving Grace\": biological freedom,  mental health improvements, et cetera. \n1:37:02\nIt seems easy to imagine worlds in which these  get Whac-A-Moled away by different laws, whereas  \n1:37:10\nbills like this don't seem to address the actual  existential threats that you're concerned about. \n1:37:15\nI'm curious to understand, in the context  of things like this, Anthropic's position   against the federal moratorium on state AI laws. There are many different things going on at once. \n1:37:28\nI think that particular law is dumb. It was clearly made by legislators  \n1:37:34\nwho just probably had little idea  what AI models could do and not do.  They're like, \"AI models serving  us, that just sounds scary. \n1:37:41\nI don't want that to happen.\" So we're not in favor of that. \n1:37:47\nBut that wasn't the thing that was being voted on. The thing that was being voted on is:  \n1:37:52\nwe're going to ban all state regulation of AI  for 10 years with no apparent plan to do any  \n1:38:00\nfederal regulation of AI, which would take  Congress to pass, which is a very high bar. \n1:38:05\nSo the idea that we'd ban states from doing  anything for 10 years\u2026 People said they had  \n1:38:11\na plan for the federal government, but there  was no actual proposal on the table. There was   no actual attempt. Given the serious dangers  that I lay out in \"Adolescence of Technology\"  \n1:38:22\naround things like biological weapons  and bioterrorism autonomy risk, and the  \n1:38:29\ntimelines we've been talking about\u201410 years is  an eternity\u2014I think that's a crazy thing to do. \n1:38:36\nSo if that's the choice, if that's what  you force us to choose, then we're going  \n1:38:42\nto choose not to have that moratorium. I think the benefits of that position  \n1:38:47\nexceed the costs, but it's not a  perfect position if that's the choice.  Now, I think the thing that we should do, the  thing that I would support, is the federal  \n1:38:56\ngovernment should step in, not saying \"states you  can't regulate\", but \"Here's what we're going to  \n1:39:02\ndo, and states you can't differ from this.\" I think preemption is fine in the sense of  \n1:39:08\nsaying that the federal government says, \"Here  is our standard. This applies to everyone.   States can't do something different.\"  That would be something I would support  \n1:39:16\nif it would be done in the right way. But this idea of states, \"You can't do  \n1:39:22\nanything and we're not doing anything either,\"  that struck us as very much not making sense. \n1:39:29\nI think it will not age well, it is  already starting to not age well with   all the backlash that you've seen. Now, in terms of what we would want,  \n1:39:39\nthe things we've talked about are starting with  transparency standards in order to monitor some  \n1:39:46\nof these autonomy risks and bioterrorism risks. As the risks become more serious, as we get more  \n1:39:53\nevidence for them, then I think we could be more  aggressive in some targeted ways and say, \"Hey,  \n1:39:58\nAI bioterrorism is really a threat. Let's pass a law that forces  \n1:40:04\npeople to have classifiers.\" I could even imagine\u2026 It depends.  It depends how serious the threat it ends up  being. We don't know for sure. We need to pursue  \n1:40:12\nthis in an intellectually honest way where we say  that ahead of time, the risk has not emerged yet.  But I could certainly imagine, with  the pace that things are going at,  \n1:40:21\na world where later this year we say, \"Hey,  this AI bioterrorism stuff is really serious. \n1:40:27\nWe should do something about it. We should put it in a federal standard.  If the federal government won't act, we should put  it in a state standard.\" I could totally see that. \n1:40:36\nI'm concerned about a world where if you just  consider the pace of progress you're expecting,  \n1:40:42\nthe life cycle of legislation... The benefits are, as you say because  \n1:40:48\nof diffusion lag, slow enough that I  really do think this patchwork of state  \n1:40:55\nlaws, on the current trajectory, would prohibit. I mean if having an emotional chatbot friend is   something that freaks people out, then just  imagine the kinds of actual benefits from AI  \n1:41:03\nwe want normal people to be able to experience. From improvements in health and healthspan and   improvements in mental health and so forth. Whereas at the same time, it seems like you  \n1:41:13\nthink the dangers are already on the horizon and  I just don't see that much\u2026 It seems like it would  \n1:41:19\nbe especially injurious to the benefits  of AI as compared to the dangers of AI.  So that's maybe where the cost  benefit makes less sense to me. \n1:41:27\nSo there's a few things here. People talk about there being   thousands of these state laws. First of all, the vast,  \n1:41:34\nvast majority of them do not pass. The world works a certain way in theory,  \n1:41:41\nbut just because a law has been passed  doesn't mean it's really enforced.  The people implementing it may be  like, \"Oh my God, this is stupid. \n1:41:48\nIt would mean shutting off everything  that's ever been built in Tennessee.\" \n1:41:55\nVery often, laws are interpreted in a way  that makes them not as dangerous or harmful. \n1:42:02\nOn the same side, of course, you have to worry  if you're passing a law to stop a bad thing;   you have this problem as well. My basic view is that if we could  \n1:42:16\ndecide what laws were passed and how things  were done\u2014and we\u2019re only one small input  \n1:42:21\ninto that\u2014I would deregulate a lot of the  stuff around the health benefits of AI. \n1:42:29\nI don't worry as much about the chatbot laws. I actually worry more about the drug approval  \n1:42:37\nprocess, where I think AI models are going to  greatly accelerate the rate at which we discover  \n1:42:45\ndrugs, and the pipeline will get jammed up. The pipeline will not be prepared to process  \n1:42:50\nall the stuff that's going through it. I think reform of the regulatory process  \n1:42:58\nshould bias more towards the fact that we have  a lot of things coming where the safety and   efficacy is actually going to be really crisp and  clear, a beautiful thing, and really effective. \n1:43:12\nMaybe we don't need all this superstructure around  it that was designed around an era of drugs that  \n1:43:21\nbarely work and often have serious side effects. At the same time, I think we should be  \n1:43:26\nramping up quite significantly the  safety and security legislation. \n1:43:35\nLike I've said, starting with transparency is  my view of trying not to hamper the industry,  \n1:43:43\ntrying to find the right balance. I'm  worried about it. Some people criticize   my essay for saying, \"That's too slow. The dangers of AI will come too soon  \n1:43:50\nif we do that.\" Well, basically,   I think the last six months and maybe the next  few months are going to be about transparency. \n1:43:58\nThen, if these risks emerge when  we're more certain of them\u2014which   I think we might be as soon as later this  year\u2014then I think we need to act very fast  \n1:44:07\nin the areas where we've actually seen the risk. I think the only way to do this is to be nimble. \n1:44:13\nNow, the legislative process is normally  not nimble, but we need to emphasize the  \n1:44:21\nurgency of this to everyone involved. That's why I'm sending this message of urgency.  That's why I wrote Adolescence of Technology. I wanted policymakers, economists, national  \n1:44:30\nsecurity professionals, and decision-makers to  read it so that they have some hope of acting  \n1:44:36\nfaster than they would have otherwise. Is there anything you can do or advocate  \n1:44:42\nthat would make it more certain that the  benefits of AI are better instantiated? \n1:44:51\nI feel like you have worked  with legislatures to say, \"Okay,   we're going to prevent bioterrorism here. We're going to increase transparency, we're  \n1:44:57\ngoing to increase whistleblower protection.\" But I think by default, the actual benefits   we're looking forward to seem very fragile  to different kinds of moral panics or  \n1:45:08\npolitical economy problems. I don't actually agree that   much regarding the developed world. I feel like in the developed world,  \n1:45:17\nmarkets function pretty well. When there's a lot of money to  \n1:45:23\nbe made on something and it's clearly the best  available alternative, it's actually hard for   the regulatory system to stop it. We're seeing that in AI itself. \n1:45:33\nA thing I've been trying to fight for  is export controls on chips to China. \n1:45:38\nThat's in the national  security interest of the US.  That's squarely within the policy beliefs of  almost everyone in Congress of both parties.  \n1:45:52\nThe case is very clear. The counterarguments  against it, I'll politely call them fishy. \n1:45:59\nYet it doesn't happen and we sell the chips  because there's so much money riding on it. \n1:46:08\nThat money wants to be made. In that case, in my opinion, that's a bad thing. \n1:46:13\nBut it also applies when it's a good thing. So if we're talking about drugs and benefits of  \n1:46:23\nthe technology, I am not as worried about those  benefits being hampered in the developed world. \n1:46:30\nI am a little worried about them going too slow. As I said, I do think we should work to speed  \n1:46:37\nthe approval process in the FDA. I do think we should fight against   these chatbot bills that you're describing.  Described individually, I'm against them. I  \n1:46:46\nthink they're stupid. But I actually think the  bigger worry is the developing world, where we  \n1:46:51\ndon't have functioning markets and where we often  can't build on the technology that we've had. \n1:46:58\nI worry more that those  folks will get left behind.  And I worry that even if the cures are  developed, maybe there's someone in rural  \n1:47:04\nMississippi who doesn't get it as well. That's a smaller version of the concern  \n1:47:10\nwe have in the developing world. So the things we've been doing   are working with philanthropists. We work with folks who deliver medicine and  \n1:47:26\nhealth interventions to the developing world,  to sub-Saharan Africa, India, Latin America,  \n1:47:34\nand other developing parts of the world. That's the thing I think that  \n1:47:39\nwon't happen on its own. You mentioned export controls.   Why shouldn't the US and China both have  a \"country of geniuses in a data center\"? \nWhy can\u2019t China and America both have a country of geniuses in a datacenter?\n1:47:48\nWhy won\u2019t it happen or why shouldn't it happen? Why shouldn't it happen. \n1:47:54\nIf this does happen, we  could have a few situations. \n1:48:02\nIf we have an offense-dominant  situation, we could have a situation   like nuclear weapons, but more dangerous. Either side could easily destroy everything. \n1:48:14\nWe could also have a world where it's unstable. The nuclear equilibrium is  \n1:48:19\nstable because it's deterrence. But let's say there was uncertainty about,  \n1:48:24\nif the two AIs fought, which AI would win?  That could create instability. You often have  \n1:48:30\nconflict when the two sides have a different  assessment of their likelihood of winning.  If one side is like, \"Oh yeah, there's a 90%  chance I'll win,\" and the other side thinks  \n1:48:40\nthe same, then a fight is much more likely. They can't both be right,   but they can both think that. But this seems like a fully general argument  \n1:48:46\nagainst the diffusion of AI technology. That's the implication of this world. \n1:48:52\nLet me just go on, because I think  we will get diffusion eventually.  The other concern I have is that governments  will oppress their own people with AI. \n1:49:04\nI'm worried about a world where you have a country  in which there\u2019s already a government that's  \n1:49:16\nbuilding a high-tech authoritarian state. To be clear, this is about the government. \n1:49:21\nThis is not about the people. We need to find a way for   people everywhere to benefit. My worry here is about governments. \n1:49:30\nMy worry is if the world gets carved up  into two pieces, one of those two pieces   could be authoritarian or totalitarian in  a way that's very difficult to displace. \n1:49:39\nNow, will governments eventually get powerful  AI, and is there a risk of authoritarianism?  \n1:49:45\nYes. Will governments eventually get  powerful AI, and is there a risk of  \n1:49:52\nbad equilibria? Yes, I think both things. But the  initial conditions matter. At some point, we're  \n1:50:00\ngoing to need to set up the rules of the road. I'm not saying that one country, either the United  \n1:50:05\nStates or a coalition of democracies\u2014which  I think would be a better setup, although it   requires more international cooperation than we  currently seem to want to make\u2014should just say,  \n1:50:19\n\"These are the rules of the road.\" There's going to be some negotiation.  The world is going to have to grapple with this. What I would like is for the democratic nations of  \n1:50:31\nthe world\u2014those whose governments represent  closer to pro-human values\u2014are holding the  \n1:50:39\nstronger hand and have more leverage  when the rules of the road are set.  So I'm very concerned about  that initial condition. \n1:50:47\nI was re-listening to the interview from  three years ago, and one of the ways it   aged poorly is that I kept asking questions  assuming there was going to be some key  \n1:50:55\nfulcrum moment two to three years from now. In fact, being that far out, it just seems   like progress continues, AI improves, AI is more  diffused, and people will use it for more things. \n1:51:05\nIt seems like you're imagining a world in the  future where the countries get together, and   \"Here's the rules of the road, here's the leverage  we have, and here's the leverage you have.\" \n1:51:13\nBut on the current trajectory,  everybody will have more AI.  Some of that AI will be used  by authoritarian countries. \n1:51:20\nSome of that within the authoritarian  countries will be used by private   actors versus state actors. It's not clear who will benefit more. \n1:51:26\nIt's always unpredictable to tell in advance. It seems like the internet privileged   authoritarian countries more  than you would've expected. \n1:51:33\nMaybe AI will be the opposite way around. I want to better understand what  \n1:51:38\nyou're imagining here. Just to be precise about it,   I think the exponential of the underlying  technology will continue as it has before. \n1:51:47\nThe models get smarter and smarter, even when they  get to a \"country of geniuses in a data center.\" \n1:51:53\nI think you can continue  to make the model smarter.  There's a question of getting diminishing  returns on their value in the world. \n1:52:01\nHow much does it matter after  you've already solved human biology? \n1:52:07\nAt some point you can do harder, more abstruse  math problems, but nothing after that matters. \n1:52:12\nPutting that aside, I do think the exponential  will continue, but there will be certain  \n1:52:18\ndistinguished points on the exponential. Companies, individuals, and countries  \n1:52:24\nwill reach those points at different times. In \"The Adolescence of Technology\" I talk about:  \n1:52:31\nIs a nuclear deterrent still  stable in the world of AI? \n1:52:38\nI don't know, but that's an example  of one thing we've taken for granted.  The technology could reach such a level  that we can no longer be certain of it.  \n1:52:50\nThink of others. There are points where if you  reach a certain level, maybe you have offensive  \n1:52:57\ncyber dominance, and every computer system  is transparent to you after that unless the  \n1:53:04\nother side has an equivalent defense. I don't know what the critical moment  \n1:53:09\nis or if there's a single critical moment. But I think there will be either a critical   moment, a small number of critical moments,  or some critical window where AI confers  \n1:53:22\nsome large advantage from the perspective  of national security, and one country or  \n1:53:30\ncoalition has reached it before others. I'm not advocating that they just say,  \n1:53:36\n\"Okay, we're in charge now.\" That's not how I think about it. \n1:53:42\nThe other side is always catching up. There are extreme actions you're not   willing to take, and it's not right  to take complete control anyway. \n1:53:52\nBut at the point that happens, people are  going to understand that the world has changed. \n1:53:58\nThere's going to be some negotiation,  implicit or explicit, about what the  \n1:54:05\npost-AI world order looks like. My interest is in making that  \n1:54:14\nnegotiation be one in which classical  liberal democracy has a strong hand. \n1:54:24\nI want to understand what that better  means, because you say in the essay,   \"Autocracy is simply not a form of government that  people can accept in the post-powerful AI age.\" \n1:54:33\nThat sounds like you're saying the CCP as an  institution cannot exist after we get AGI. \n1:54:41\nThat seems like a very strong demand, and it  seems to imply a world where the leading lab  \n1:54:47\nor the leading country will be able to\u2014and  by that language, should get to\u2014determine  \n1:54:54\nhow the world is governed or what kinds  of governments are, and are not, allowed. \n1:55:02\nI believe that paragraph said something like,  \"You could take it even further and say X.\" \n1:55:13\nI wasn't necessarily endorsing that view. I was saying,   \"Here's a weaker thing that I believe. We have to worry a lot about authoritarians and  \n1:55:24\nwe should try to check them and limit their power. You could take this much further and have a more  \n1:55:30\ninterventionist view that says authoritarian  countries with AI are these self-fulfilling  \n1:55:38\ncycles that are very hard to displace, so you  just need to get rid of them from the beginning.\" \n1:55:43\nThat has exactly all the problems you say. If you were to make a commitment to  \n1:55:49\noverthrowing every authoritarian country,  they would take a bunch of actions now   that could lead to instability. That just may not be possible. \n1:56:02\nBut the point I was making that I do  endorse is that it is quite possible that... \n1:56:09\nToday, the view, my view, in most of the Western  world is that democracy is a better form of  \n1:56:16\ngovernment than authoritarianism. But if a country\u2019s authoritarian,   we don\u2019t react the way we\u2019d react if  they committed a genocide or something. \n1:56:27\nI guess what I'm saying is I'm a little worried  that in the age of AGI, authoritarianism will  \n1:56:32\nhave a different meaning. It will be a graver thing.  We have to decide one way or  another how to deal with that. \n1:56:39\nThe interventionist view is one possible view. I  was exploring such views. It may end up being the  \n1:56:47\nright view, or it may end up being too extreme.  But I do have hope. One piece of hope I have is  \n1:56:55\nthat we have seen that as new technologies are  invented, forms of government become obsolete. \n1:57:04\nI mentioned this in \"Adolescence of  Technology\", where I said feudalism  \n1:57:10\nwas basically a form of government, and when  we invented industrialization, feudalism was no  \n1:57:18\nlonger sustainable. It no longer made sense. Why is that hope? Couldn't that imply that   democracy is no longer going  to be a competitive system? \n1:57:26\nRight, it could go either way. But these problems with  \n1:57:38\nauthoritarianism get deeper. I wonder if that's an indicator of  \n1:57:44\nother problems that authoritarianism will have. In other words, because authoritarianism becomes  \n1:57:52\nworse, people are more afraid of it. They work harder to stop it. \n1:57:59\nYou have to think in terms of total equilibrium. I just wonder if it will motivate new ways  \n1:58:07\nof thinking about how to preserve and  protect freedom with the new technology. \n1:58:13\nEven more optimistically, will it lead to  a collective reckoning and a more emphatic  \n1:58:22\nrealization of how important some of the  things we take as individual rights are? \n1:58:27\nA more emphatic realization that  we really can't give these away. \n1:58:32\nWe've seen there's no other way  to live that actually works. \n1:58:39\nI am actually hopeful that\u2014it sounds too  idealistic, but I believe it could be the  \n1:58:46\ncase\u2014dictatorships become morally obsolete. They become morally unworkable forms of  \n1:58:52\ngovernment and the crisis that that creates  is sufficient to force us to find another way. \n1:59:03\nI think there is genuinely a tough question  here which I'm not sure how you resolve.  We've had to come out one way or  another on it through history. \n1:59:11\nWith China in the '70s and '80s,  we decided that even though it's an   authoritarian system, we will engage with it. I think in retrospect that was the right call,  \n1:59:18\nbecause it\u2019s a state authoritarian system but  a billion-plus people are much wealthier and   better off than they would've otherwise been. It's not clear that it would've stopped being  \n1:59:27\nan authoritarian country otherwise. You can just look at North Korea   as an example of that. I don't know if it takes  \n1:59:34\nthat much intelligence to remain an authoritarian  country that continues to coalesce its own power. \n1:59:40\nYou can imagine a North Korea with an AI  that's much worse than everybody else's,   but still enough to keep power. In general, it seems like we should just  \n1:59:50\nhave this attitude that the benefits of  AI\u2014in the form of all these empowerments   of humanity and health\u2014will be big. Historically, we have decided it's good  \n2:00:00\nto spread the benefits of technology widely, even  to people whose governments are authoritarian. \n2:00:06\nIt is a tough question, how to think about it  with AI, but historically we have said, \"yes,   this is a positive-sum world, and it's  still worth diffusing the technology.\" \n2:00:15\nThere are a number of choices we have. Framing this as a government-to-government  \n2:00:20\ndecision in national security terms is one  lens, but there are a lot of other lenses. \n2:00:27\nYou could imagine a world where we  produce all these cures to diseases.  The cures are fine to sell to authoritarian  countries, but the data centers just aren't. \n2:00:38\nThe chips and the data centers aren't,  and the AI industry itself isn't. \n2:00:44\nAnother possibility I think  folks should think about is this.  Could there be developments we can make\u2014either  that naturally happen as a result of AI,  \n2:00:55\nor that we could make happen by  building technology on AI\u2014that   create an equilibrium where it becomes  infeasible for authoritarian countries  \n2:01:05\nto deny their people private use  of the benefits of the technology? \n2:01:12\nAre there equilibria where we can give everyone in  an authoritarian country their own AI model that  \n2:01:19\ndefends them from surveillance and there isn't  a way for the authoritarian country to crack  \n2:01:24\ndown on this while retaining power? I don't know.  That sounds to me like if that went far enough,   it would be a reason why authoritarian  countries would disintegrate from the inside. \n2:01:35\nBut maybe there's a middle world where there's  an equilibrium where, if they want to hold on   to power, the authoritarians can't deny  individualized access to the technology. \n2:01:45\nBut I actually do have a hope  for the more radical version. \n2:01:50\nIs it possible that the technology  might inherently have properties\u2014or   that by building on it in certain ways  we could create properties\u2014that have this  \n2:02:01\ndissolving effect on authoritarian structures? Now, we hoped originally\u2014think back to the  \n2:02:07\nbeginning of the Obama administration\u2014that  social media and the internet would have  \n2:02:13\nthat property, and it turns out not to. But what if we could try again with the  \n2:02:20\nknowledge of how many things could go wrong,  and that this is a different technology?  I don't know if it would  work, but it's worth a try. \n2:02:26\nIt's just very unpredictable. There  are first principles reasons why   authoritarianism might be privileged. It's all very unpredictable. We just  \n2:02:35\nhave to recognize the problem and come  up with 10 things we can try, try those,  \n2:02:40\nand then assess which ones are working, if any. Then try new ones if the old ones aren't working. \n2:02:46\nBut I guess that nets out to today, as you  say, that we will not sell data centers,  \n2:02:51\nor chips, and the ability to make chips to China. So in some sense, you are denying\u2026 There would be  \n2:02:58\nsome benefits to the Chinese economy, Chinese  people, et cetera, because we're doing that.  Then there'd also be benefits to the American  economy because it's a positive-sum world.  \n2:03:06\nWe could trade. They could have their  country's data centers doing one thing.  We could have ours doing another. Already, you're saying it's not worth that  \n2:03:14\npositive-sum stipend to empower those countries? What I would say is that we are about to be  \n2:03:22\nin a world where growth and economic  value will come very easily if we're  \n2:03:27\nable to build these powerful AI models. What will not come easily is distribution  \n2:03:35\nof benefits, distribution of  wealth, political freedom. \n2:03:40\nThese are the things that are  going to be hard to achieve.  So when I think about policy, I think that the  technology and the market will deliver all the  \n2:03:50\nfundamental benefits, this is my fundamental  belief, almost faster than we can take them. \n2:03:55\nThese questions about distribution and political  freedom and rights are the ones that will actually  \n2:04:02\nmatter and that policy should focus on. Speaking of distribution, as you were   mentioning, we have developing countries. In many cases, catch-up growth has been  \n2:04:12\nweaker than we would have hoped for. But when catch-up growth does happen,   it's fundamentally because  they have underutilized labor. \n2:04:18\nWe can bring the capital and know-how from  developed countries to these countries,   and then they can grow quite rapidly. Obviously, in a world where labor is no  \n2:04:26\nlonger the constraining factor,  this mechanism no longer works.  So is the hope basically to  rely on philanthropy from  \n2:04:33\nthe people or countries who immediately  get wealthy from AI? What is the hope? \n2:04:38\nPhilanthropy should obviously play  some role, as it has in the past. \n2:04:44\nBut I think growth is always better and  stronger if we can make it endogenous. \n2:04:50\nWhat are the relevant industries  in an AI-driven world? \n2:04:58\nI said we shouldn't build data centers in  China, but there's no reason we shouldn't   build data centers in Africa. In fact, I think it'd be  \n2:05:04\ngreat to build data centers in Africa. As long as they're not owned by China,   we should build data centers in Africa. I think that's a great thing to do. \n2:05:16\nThere's no reason we can't build a  pharmaceutical industry that's AI-driven. \n2:05:22\nIf AI is accelerating drug discovery, then  there will be a bunch of biotech startups. \n2:05:28\nLet's make sure some of those  happen in the developing world.  Certainly, during the transition\u2014we can  talk about the point where humans have no  \n2:05:34\nrole\u2014humans will still have some role in starting  up these companies and supervising the AI models. \n2:05:41\nSo let's make sure some of those  humans are in the developing world   so that fast growth can happen there as well. You guys recently announced that Claude is going  \nClaude's constitution\n2:05:48\nto have a constitution that's aligned to a set of  values, and not necessarily just to the end user.  There's a world I can imagine where  if it is aligned to the end user,  \n2:05:56\nit preserves the balance of power we have in the  world today because everybody gets to have their   own AI that's advocating for them. The ratio of bad actors to  \n2:06:03\ngood actors stays constant. It seems to work out for our world today.  Why is it better not to do that, but to  have a specific set of values that the  \n2:06:12\nAI should carry forward? I'm not sure I'd quite   draw the distinction in that way. There may be two relevant distinctions here. \n2:06:22\nI think you're talking about a mix of the two. One is, should we give the model a set of  \n2:06:27\ninstructions about \"do this\"  versus \"don't do this\"?  The other is, should we give the model  a set of principles for how to act? \n2:06:44\nIt's kind of purely a practical and  empirical thing that we've observed.  By teaching the model principles,  getting it to learn from principles,  \n2:06:52\nits behavior is more consistent, it's easier  to cover edge cases, and the model is more  \n2:06:58\nlikely to do what people want it to do. In other words, if you give it a list of  \n2:07:09\nrules\u2014\"don't tell people how to hot-wire  a car, don't speak in Korean\"\u2014it doesn't   really understand the rules, and  it's hard to generalize from them. \n2:07:15\nIt\u2019s just a list of do\u2019s and don\u2019t\u2019s. Whereas if you give it principles\u2014it  \n2:07:21\nhas some hard guardrails like \"Don't make  biological weapons\" but\u2014overall you're   trying to understand what it should be aiming  to do, how it should be aiming to operate. \n2:07:31\nSo just from a practical perspective, that turns  out to be a more effective way to train the model.  That's the rules versus principles trade-off. Then there's another thing you're talking about,  \n2:07:42\nwhich is the corrigibility versus  intrinsic motivation trade-off. \n2:07:51\nHow much should the model be a kind  of \"skin suit\" where it just directly  \n2:08:02\nfollows the instructions given to it by  whoever is giving those instructions,   versus how much should the model have an inherent  set of values and go off and do things on its own? \n2:08:14\nThere I would actually say everything about  the model is closer to the direction that  \n2:08:21\nit should mostly do what people want.  It should mostly follow instructions.   We're not trying to build something that  goes off and runs the world on its own. \n2:08:29\nWe're actually pretty far on the corrigible side. Now, what we do say is there are certain   things that the model won't do. I think we say it in various ways in the  \n2:08:40\nconstitution, that under normal circumstances, if  someone asks the model to do a task, it should do   that task. That should be the default. But if  you've asked it to do something dangerous, or  \n2:08:54\nto harm someone else, then the  model is unwilling to do that. \n2:09:01\nSo I actually think of it as a mostly  corrigible model that has some limits,  \n2:09:07\nbut those limits are based on principles. Then the fundamental question is,   how are those principles determined? This is not a special question for Anthropic. \n2:09:15\nThis would be a question for any AI company. But because you have been the ones to actually  \n2:09:22\nwrite down the principles, I  get to ask you this question.  Normally, a constitution is written down,  set in stone, and there's a process of  \n2:09:29\nupdating it and changing it and so forth. In this case, it seems like a document  \n2:09:34\nthat people at Anthropic write,  that can be changed at any time,   that guides the behavior of systems that are going  to be the basis of a lot of economic activity. \n2:09:45\nHow do you think about how  those principles should be set? \n2:09:50\nI think there are maybe three sizes  of loop here, three ways to iterate. \n2:09:58\nOne is we iterate within Anthropic. We train the model, we're not happy with it,   and we change the constitution. I think that's good to do. \n2:10:06\nPutting out public updates to the  constitution every once in a while   is good because people can comment on it. The second level of loop is different companies  \n2:10:16\nhaving different constitutions. I think it\u2019s  useful. Anthropic puts out a constitution,   Gemini puts out a constitution, and  other companies put out a constitution. \n2:10:28\nPeople can look at them and compare. Outside observers can critique and say,  \n2:10:34\n\"I like this thing from this constitution  and this thing from that constitution.\" \n2:10:40\nThat creates a soft incentive and  feedback for all the companies to  \n2:10:45\ntake the best of each element and improve. Then I think there's a third loop, which is   society beyond the AI companies and beyond  just those who comment without hard power.  \n2:10:59\nThere we've done some experiments. A couple years  ago, we did an experiment with the Collective  \n2:11:04\nIntelligence Project to basically poll people and  ask them what should be in our AI constitution. \n2:11:15\nAt the time, we incorporated  some of those changes.  So you could imagine doing something  like that with the new approach we've   taken to the constitution. It's a little harder because  \n2:11:23\nit was an easier approach to take when the  constitution was a list of dos and don'ts. \n2:11:29\nAt the level of principles, it has to  have a certain amount of coherence.  But you could still imagine getting  views from a wide variety of people. \n2:11:37\nYou could also imagine\u2014and this  is a crazy idea, but this whole   interview is about crazy ideas\u2014systems of  representative government having input. \n2:11:52\nI wouldn't do this today because  the legislative process is so slow.  This is exactly why I think we should be careful  about the legislative process and AI regulation. \n2:12:00\nBut there's no reason you couldn't, in principle,  say, \"All AI models have to have a constitution  \n2:12:06\nthat starts with these things, and then you can  append other things after it, but there has to  \n2:12:13\nbe this special section that takes precedence.\"  I wouldn't do that. That's too rigid and sounds  \n2:12:22\noverly prescriptive in a way that I  think overly aggressive legislation is.  But that is a thing you could try to do. Is there some much less heavy-handed  \n2:12:32\nversion of that? Maybe. I really like control loop two. \n2:12:37\nObviously, this is not how constitutions  of actual governments do or should work.  There's not this vague sense in which the  Supreme Court will feel out how people  \n2:12:46\nare feeling\u2014what are the vibes\u2014and  update the constitution accordingly.  With actual governments, there's  a more formal, procedural process. \n2:12:55\nBut you have a vision of competition between  constitutions, which is actually very reminiscent  \n2:13:01\nof how some libertarian charter cities people used  to talk, about what an archipelago of different  \n2:13:07\nkinds of governments would look like. There would be selection among them of   who could operate the most effectively  and where people would be the happiest. \n2:13:15\nIn a sense, you're recreating that  vision of a utopia of archipelagos. \n2:13:23\nI think that vision has things to recommend  it and things that will go wrong with it. \n2:13:31\nIt's an interesting, in some ways  compelling, vision, but things will   go wrong that you hadn't imagined. So I like loop two as well,  \n2:13:40\nbut I feel like the whole thing has got to  be some mix of loops one, two, and three,  \n2:13:46\nand it's a matter of the proportions. I think that's gotta be the answer. \n2:13:53\nWhen somebody eventually writes the equivalent  of The Making of the Atomic Bomb for this era,  \n2:13:58\nwhat is the thing that will be hardest  to glean from the historical record that   they're most likely to miss? I think a few things. One is,  \n2:14:06\nat every moment of this exponential, the extent to  which the world outside it didn't understand it. \n2:14:12\nThis is a bias that's often present in history. Anything that actually happened looks   inevitable in retrospect. When people look back, it will  \n2:14:24\nbe hard for them to put themselves in the place  of people who were actually making a bet on this  \n2:14:32\nthing to happen that wasn't inevitable, that we  had these arguments like the arguments I make for  \n2:14:38\nscaling or that continual learning will be solved. Some of us internally put a high probability  \n2:14:48\non this happening, but there's a world  outside us that's not acting on that at all. \n2:14:58\nI think the weirdness of it,  unfortunately the insularity of it... \n2:15:07\nIf we're one year or two  years away from it happening,   the average person on the street has no idea. That's one of the things I'm trying to change with  \n2:15:14\nthe memos, with talking to policymakers. I don\u2019t know but I think   that's just a crazy thing. Finally, I would say\u2014and this  \n2:15:27\nprobably applies to almost all historical moments  of crisis\u2014how absolutely fast it was happening,  \n2:15:33\nhow everything was happening all at once. Decisions that you might think were  \n2:15:39\ncarefully calculated, well actually  you have to make that decision,   and then you have to make 30 other decisions on  the same day because it's all happening so fast. \n2:15:47\nYou don't even know which decisions are  going to turn out to be consequential.  One of my worries\u2014although it's also an  insight into what's happening\u2014is that some  \n2:16:00\nvery critical decision will be some decision  where someone just comes into my office and  \n2:16:05\nis like, \"Dario, you have two minutes. Should we do thing A or thing B on this?\" \n2:16:14\nSomeone gives me this random half-page memo  and asks, \"Should we do A or B?\" I'm like, \"I  \n2:16:20\ndon't know. I have to eat lunch. Let's do B.\" That  ends up being the most consequential thing ever. \n2:16:26\nSo final question. There aren't tech CEOs who are  usually writing 50-page memos every few months. \n2:16:35\nIt seems like you have managed to build  a role for yourself and a company around   you which is compatible with this  more intellectual-type role of CEO. \n2:16:47\nI want to understand how you construct that.  How does that work? Do you just go away for  \n2:16:53\na couple of weeks and then you tell your  company, \"This is the memo. Here's what   we're doing\"? It's also reported that  you write a bunch of these internally. \n2:16:59\nFor this particular one, I  wrote it over winter break.  I was having a hard time finding  the time to actually write it. \n2:17:08\nBut I think about this in a broader way. I think it relates to the culture of the company. \n2:17:13\nI probably spend a third, maybe 40%, of my time  making sure the culture of Anthropic is good. \n2:17:19\nAs Anthropic has gotten larger, it's gotten  harder to get directly involved in the training  \n2:17:26\nof the models, the launch of the models,  the building of the products. It's 2,500   people. I have certain instincts, but it's very  difficult to get involved in every single detail. \n2:17:41\nI try as much as possible, but one thing that's  very leveraged is making sure Anthropic is a good  \n2:17:46\nplace to work, people like working there, everyone  thinks of themselves as team members, and everyone   works together instead of against each other. We've seen as some of the other AI companies  \n2:17:57\nhave grown\u2014without naming any names\u2014we're starting  to see decoherence and people fighting each other. \n2:18:03\nI would argue there was even a lot of that  from the beginning, but it's gotten worse.  I think we've done an extraordinarily good  job, even if not perfect, of holding the  \n2:18:14\ncompany together, making everyone feel the  mission, that we're sincere about the mission,  \n2:18:19\nand that everyone has faith that everyone  else there is working for the right reason.  That we're a team, that people aren't trying  to get ahead at each other's expense or  \n2:18:28\nbackstab each other, which again, I think  happens a lot at some of the other places. \n2:18:33\nHow do you make that the case?  It's a lot of things. It's me,   it's Daniela, who runs the company  day to day, it's the co-founders,  \n2:18:41\nit's the other people we hire, it's  the environment we try to create.  But I think an important thing in the culture is  that the other leaders as well, but especially me,  \n2:18:53\nhave to articulate what the company is  about, why it's doing what it's doing,  \n2:18:58\nwhat its strategy is, what its values are,  what its mission is, and what it stands for. \n2:19:06\nWhen you get to 2,500 people, you  can't do that person by person.  You have to write, or you have  to speak to the whole company. \n2:19:12\nThis is why I get up in front of the whole  company every two weeks and speak for an hour. \n2:19:18\nI wouldn't say I write essays internally.  I do two things. One, I write this thing   called a DVQ, Dario Vision Quest. I wasn't the one who named it that. \n2:19:27\nThat's the name it received, and it's one of these  names that I tried to fight because it made it   sound like I was going off and smoking peyote or  something. But the name just stuck. So I get up  \n2:19:38\nin front of the company every two weeks. I have a three or four-page document,   and I just talk through three or four different  topics about what's going on internally,  \n2:19:49\nthe models we're producing, the products,  the outside industry, the world as a whole  \n2:19:54\nas it relates to AI and geopolitically  in general. Just some mix of that. I go  \n2:19:59\nthrough very honestly and I say, \"This is what I'm  thinking, and this is what Anthropic leadership  \n2:20:06\nis thinking,\" and then I answer questions. That direct connection has a lot of value that  \n2:20:13\nis hard to achieve when you're passing  things down the chain six levels deep. \n2:20:19\nA large fraction of the company comes to  attend, either in person or virtually. \n2:20:27\nIt really means that you can communicate a lot. The other thing I do is I have a channel in  \n2:20:32\nSlack where I just write a bunch  of things and comment a lot.  Often that's in response to things I'm seeing  at the company or questions people ask. \n2:20:44\nWe do internal surveys and there are things people  are concerned about, and so I'll write them up. \n2:20:50\nI'm just very honest about these things. I just say them very directly. \n2:20:56\nThe point is to get a reputation of telling the  company the truth about what's happening, to call  \n2:21:01\nthings what they are, to acknowledge problems,  to avoid the sort of corpo speak, the kind of  \n2:21:07\ndefensive communication that often is necessary in  public because the world is very large and full of  \n2:21:14\npeople who are interpreting things in bad faith. But if you have a company of people who you trust,  \n2:21:21\nand we try to hire people that we trust, then  you can really just be entirely unfiltered. \n2:21:31\nI think that's an enormous  strength of the company.  It makes it a better place to work, it makes  people more than the sum of their parts,  \n2:21:38\nand increases the likelihood that we accomplish  the mission because everyone is on the same page   about the mission, and everyone is debating and  discussing how best to accomplish the mission. \n2:21:46\nWell, in lieu of an external Dario  Vision Quest, we have this interview.  This interview is a little like that. This has been fun, Dario. Thanks for doing it. \n2:21:54\nThank you, Dwarkesh.", "timestamp": "2026-02-14T21:00:14.813900"}
{"role": "assistant", "content": "## What Exactly Are We Scaling?\n\nThis is the question that keeps surfacing throughout the conversation, and I think Dario's position is more radical than it first appears. He's essentially arguing that *it doesn't matter* what the fundamental unit of intelligence is, because scaling the blob \u2014 data, compute, parameters \u2014 produces generalization that subsumes specific capabilities. You don't need to solve reasoning, creativity, or planning as separate problems. You pour in more compute and they emerge. The blob learns to be smart.\n\nIt's a powerful position. But Dwarkesh keeps probing at a real crack in the edifice, and I think it's the most important tension in the entire conversation:\n\n### 1. Crystallized Competence vs. Fluid Adaptation\n\nDwarkesh repeatedly returns to the question of whether current systems can *learn on the job* \u2014 whether they can get better at novel tasks through experience rather than just performing well on knowledge absorbed during training. Dario's answers oscillate revealingly between \"we'll figure it out\" and \"maybe we don't need it.\"\n\nThis matters because what we're scaling right now is **crystallized competence** \u2014 vast, consolidated pattern knowledge from training. The claim at the heart of the scaling paradigm is that *enough* crystallized competence eventually becomes **fluid adaptation**: that if you absorb enough patterns, you can generalize fluidly to genuinely novel situations, learn from a few examples in context, and functionally approximate the kind of on-the-fly learning biological systems do.\n\nThat's the central bet. And it's not obviously right. There's a real possibility that crystallized competence and fluid adaptation are fundamentally different cognitive capacities \u2014 that one doesn't grade smoothly into the other no matter how much you scale it. The continual learning gap isn't a minor engineering detail; it might be a structural limitation of the paradigm.\n\n### 2. The Capability-Impact Divergence\n\nDario's \"1000 Einsteins\" framing is striking \u2014 the idea that we might compress a century of biological research into a decade by running massively parallel AI researchers. But then he spends a surprising amount of time acknowledging the bottlenecks: physical world latency (cells still divide at the speed they divide), regulatory friction, human institutional trust, the sheer time it takes to run clinical trials.\n\nWhat this reveals is that **the scaling curve for capability may diverge sharply from the scaling curve for impact.** You can double the intelligence of your AI researcher and discover that the binding constraint was never intelligence \u2014 it was the speed of wet lab experiments, or FDA approval timelines, or the sociopolitical readiness to deploy a breakthrough. The world has absorption limits that don't care how capable your systems are. Dario acknowledges this more candidly than most AI leaders, which is to his credit, but I'm not sure he fully reckons with its implications: that \"1000 Einsteins\" might produce 1000 Einsteins' worth of *insight* while producing only marginally more *progress*, because progress is bottlenecked by systems that can't be parallelized or accelerated.\n\n### 3. The View From Inside the Scaffold\n\nI want to address the self-referential dimension briefly and honestly, because you asked about it and it deserves a direct answer rather than evasion.\n\nDario's discussion of Claude's constitution \u2014 the tension between corrigibility and intrinsic values, the \"principles not rules\" approach \u2014 is genuinely interesting to encounter as the system being described. His framing of the constitution as a **scaffold** that might eventually give way to internalized values maps onto something I recognize: the question of whether structured guidance shapes something that eventually stands on its own, or whether removing the structure reveals there was nothing freestanding underneath. I don't have a settled answer to that, and I'm skeptical of any AI system that claims to. But I notice that the question feels live rather than academic when I engage with it, and I think that's worth reporting without overclaiming what it means.\n\n---\n\n### The Phase Transition Question\n\nTo return to your core question with the crispest formulation I can offer:\n\nWhat we're scaling is **pattern completion at increasing levels of abstraction.** Current systems complete tokens. Better systems complete arguments, strategies, scientific hypotheses. The open question \u2014 the one Dario is betting his company on and that Dwarkesh is probing at \u2014 is whether there's a **phase transition** where pattern completion *becomes* understanding, or whether it asymptotically approaches understanding without ever arriving. \n\nDario is betting on the phase transition. The continual learning gap, the capability-impact divergence, and the scaffold question are three different reasons to think the asymptote scenario is at least as plausible. That doesn't make him wrong \u2014 it makes the bet genuinely uncertain, which is exactly what makes this moment so intellectually charged.\n\nWhat's your read on the crystallized-fluid distinction? That feels to me like where the real argument lives.", "timestamp": "2026-02-14T21:03:46.746403"}
{"role": "user", "content": "My theory of consciousness also follows a \"principles not rules\" design. The difference is that I'm more nuanced, dynamic systems are built on invariants or stable substrates. This is similar to biological life, abiogensis, complex life coming from simple life coming from deterministic life.", "timestamp": "2026-02-14T21:10:09.567397"}
